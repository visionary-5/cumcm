# NIPTé—®é¢˜2ï¼šè‡ªé€‚åº”BMIåˆ†ç»„ä¸æœ€ä½³æ£€æµ‹æ—¶ç‚¹ä¼˜åŒ–
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.optimize import minimize_scalar
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
import traceback
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.patches as mpatches
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç¾åŒ–æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (15, 10)
plt.style.use('seaborn-v0_8-whitegrid')  # ä½¿ç”¨æ›´ç¾è§‚çš„æ ·å¼
sns.set_palette("husl")  # è®¾ç½®ç¾è§‚çš„é¢œè‰²è°ƒè‰²æ¿

class AdaptiveNIPTOptimizer:
    """è‡ªé€‚åº”NIPTä¼˜åŒ–å™¨"""
    
    def __init__(self, data_file='é™„ä»¶.xlsx'):
        self.data_file = data_file
        self.threshold = 0.04
        self.alpha = 0.1  # 90%ç½®ä¿¡æ°´å¹³
        
    def load_and_process_data(self):
        """åŠ è½½å¹¶å¤„ç†æ•°æ®"""
        print("="*80)
        print("æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
        print("="*80)
        
        try:
            # è¯»å–åŸå§‹æ•°æ®
            original_data = pd.read_excel(self.data_file, sheet_name=0)
            print(f"æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŸå§‹æ•°æ®å½¢çŠ¶: {original_data.shape}")
            
            # è½¬æ¢å­•å‘¨æ ¼å¼
            def convert_gestation_week(week_str):
                if pd.isna(week_str):
                    return np.nan
                week_str = str(week_str).strip()
                
                import re
                pattern = r'(\d+)(?:w\+?(\d+))?|(\d+\.\d+)'
                match = re.search(pattern, week_str.lower())
                
                if match:
                    if match.group(3):
                        return float(match.group(3))
                    else:
                        weeks = int(match.group(1))
                        days = int(match.group(2)) if match.group(2) else 0
                        return weeks + days/7
                
                try:
                    return float(week_str)
                except ValueError:
                    return np.nan
            
            original_data['å­•å‘¨_æ•°å€¼'] = original_data['æ£€æµ‹å­•å‘¨'].apply(convert_gestation_week)
            
            # è¿‡æ»¤ç”·èƒæ•°æ®
            male_data = original_data[original_data['YæŸ“è‰²ä½“æµ“åº¦'] > 0].copy()
            male_data = male_data.dropna(subset=['å­•å‘¨_æ•°å€¼', 'å­•å¦‡BMI', 'YæŸ“è‰²ä½“æµ“åº¦'])
            
            print(f"ç”·èƒæ•°æ®ç­›é€‰å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬: {len(male_data)}")
            
            # æ„å»ºç”Ÿå­˜æ•°æ®
            survival_data = []
            
            for woman_code in male_data['å­•å¦‡ä»£ç '].unique():
                woman_data = male_data[male_data['å­•å¦‡ä»£ç '] == woman_code].copy()
                woman_data = woman_data.sort_values('å­•å‘¨_æ•°å€¼')
                
                if len(woman_data) == 0:
                    continue
                
                bmi = woman_data['å­•å¦‡BMI'].iloc[0]
                age = woman_data['å¹´é¾„'].iloc[0] if 'å¹´é¾„' in woman_data.columns else np.nan
                
                # ç¡®å®šäº‹ä»¶æ—¶é—´å’Œåˆ å¤±çŠ¶æ€
                reaching_records = woman_data[woman_data['YæŸ“è‰²ä½“æµ“åº¦'] >= self.threshold]
                
                if len(reaching_records) > 0:
                    # è§‚å¯Ÿåˆ°äº‹ä»¶ï¼ˆè¾¾æ ‡ï¼‰
                    event_time = reaching_records['å­•å‘¨_æ•°å€¼'].iloc[0]
                    censored = 0
                    event_observed = 1
                else:
                    # å³åˆ å¤±
                    event_time = woman_data['å­•å‘¨_æ•°å€¼'].max()
                    censored = 1
                    event_observed = 0
                
                # æ£€æŸ¥åŒºé—´åˆ å¤±
                interval_censored = 0
                lower_bound = event_time
                upper_bound = event_time
                
                if event_observed == 1 and len(woman_data) > 1:
                    prev_records = woman_data[woman_data['å­•å‘¨_æ•°å€¼'] < event_time]
                    if len(prev_records) > 0:
                        last_below = prev_records.iloc[-1]
                        if last_below['YæŸ“è‰²ä½“æµ“åº¦'] < self.threshold:
                            interval_censored = 1
                            lower_bound = last_below['å­•å‘¨_æ•°å€¼']
                            upper_bound = event_time
                
                survival_data.append({
                    'å­•å¦‡ä»£ç ': woman_code,
                    'BMI': bmi,
                    'å¹´é¾„': age,
                    'äº‹ä»¶æ—¶é—´': event_time,
                    'äº‹ä»¶è§‚å¯Ÿ': event_observed,
                    'å³åˆ å¤±': censored,
                    'åŒºé—´åˆ å¤±': interval_censored,
                    'ä¸‹ç•Œ': lower_bound,
                    'ä¸Šç•Œ': upper_bound,
                    'æœ€å¤§æµ“åº¦': woman_data['YæŸ“è‰²ä½“æµ“åº¦'].max(),
                    'æ£€æµ‹æ¬¡æ•°': len(woman_data)
                })
            
            self.survival_df = pd.DataFrame(survival_data)
            self.original_male_data = male_data
            
            print(f"ç”Ÿå­˜æ•°æ®æ„å»ºå®Œæˆ:")
            print(f"æ€»å­•å¦‡æ•°: {len(self.survival_df)}")
            print(f"è§‚å¯Ÿåˆ°è¾¾æ ‡äº‹ä»¶: {self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].sum()}")
            print(f"å³åˆ å¤±: {self.survival_df['å³åˆ å¤±'].sum()}")
            print(f"åŒºé—´åˆ å¤±: {self.survival_df['åŒºé—´åˆ å¤±'].sum()}")
            print(f"è¾¾æ ‡ç‡: {self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].mean():.1%}")
            
            return True
            
        except Exception as e:
            print(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def analyze_bmi_concentration_relationship(self):
        """åˆ†æBMIä¸YæŸ“è‰²ä½“æµ“åº¦çš„å…³ç³»ï¼Œå¯»æ‰¾çœŸå®çš„å€’Uå‹å…³ç³»"""
        print("\n" + "="*60)
        print("æ­¥éª¤1: åˆ†æBMIä¸YæŸ“è‰²ä½“æµ“åº¦çš„çœŸå®å…³ç³»")
        print("="*60)
        
        # ä½¿ç”¨åŸå§‹æ•°æ®åˆ†æBMIä¸æµ“åº¦çš„å…³ç³»
        analysis_data = self.original_male_data.copy()
        
        print(f"åˆ†ææ ·æœ¬æ•°: {len(analysis_data)}")
        print(f"BMIèŒƒå›´: [{analysis_data['å­•å¦‡BMI'].min():.1f}, {analysis_data['å­•å¦‡BMI'].max():.1f}]")
        print(f"YæŸ“è‰²ä½“æµ“åº¦èŒƒå›´: [{analysis_data['YæŸ“è‰²ä½“æµ“åº¦'].min():.4f}, {analysis_data['YæŸ“è‰²ä½“æµ“åº¦'].max():.4f}]")
        
        # 1. åˆ†ææ¯ä¸ªBMIæ•´æ•°å€¼çš„å¹³å‡æµ“åº¦
        bmi_concentration_stats = []
        for bmi_val in range(int(analysis_data['å­•å¦‡BMI'].min()), int(analysis_data['å­•å¦‡BMI'].max()) + 1):
            bmi_data = analysis_data[(analysis_data['å­•å¦‡BMI'] >= bmi_val) & 
                                   (analysis_data['å­•å¦‡BMI'] < bmi_val + 1)]
            if len(bmi_data) >= 5:  # è‡³å°‘5ä¸ªæ ·æœ¬
                stats_dict = {
                    'BMIåŒºé—´': f'[{bmi_val}, {bmi_val+1})',
                    'BMIä¸­å¿ƒ': bmi_val + 0.5,
                    'æ ·æœ¬æ•°': len(bmi_data),
                    'å¹³å‡æµ“åº¦': bmi_data['YæŸ“è‰²ä½“æµ“åº¦'].mean(),
                    'æµ“åº¦ä¸­ä½æ•°': bmi_data['YæŸ“è‰²ä½“æµ“åº¦'].median(),
                    'æµ“åº¦æ ‡å‡†å·®': bmi_data['YæŸ“è‰²ä½“æµ“åº¦'].std(),
                    'è¾¾æ ‡ç‡': (bmi_data['YæŸ“è‰²ä½“æµ“åº¦'] >= self.threshold).mean()
                }
                bmi_concentration_stats.append(stats_dict)
                print(f"BMI {bmi_val}-{bmi_val+1}: æ ·æœ¬æ•°={len(bmi_data)}, "
                      f"å¹³å‡æµ“åº¦={stats_dict['å¹³å‡æµ“åº¦']:.4f}, è¾¾æ ‡ç‡={stats_dict['è¾¾æ ‡ç‡']:.1%}")
        
        self.bmi_concentration_stats = pd.DataFrame(bmi_concentration_stats)
        
        # 2. å¯»æ‰¾æµ“åº¦å³°å€¼ï¼ŒéªŒè¯å€’Uå‹å…³ç³»
        if len(self.bmi_concentration_stats) > 0:
            max_concentration_idx = self.bmi_concentration_stats['å¹³å‡æµ“åº¦'].idxmax()
            peak_bmi = self.bmi_concentration_stats.loc[max_concentration_idx, 'BMIä¸­å¿ƒ']
            peak_concentration = self.bmi_concentration_stats.loc[max_concentration_idx, 'å¹³å‡æµ“åº¦']
            
            print(f"\nå…³é”®å‘ç°:")
            print(f"YæŸ“è‰²ä½“æµ“åº¦å³°å€¼BMI: {peak_bmi:.1f}")
            print(f"å³°å€¼æµ“åº¦: {peak_concentration:.4f}")
            
            # éªŒè¯å€’Uå‹å…³ç³»
            correlation_left = None
            correlation_right = None
            
            left_data = self.bmi_concentration_stats[self.bmi_concentration_stats['BMIä¸­å¿ƒ'] <= peak_bmi]
            right_data = self.bmi_concentration_stats[self.bmi_concentration_stats['BMIä¸­å¿ƒ'] >= peak_bmi]
            
            if len(left_data) >= 3:
                correlation_left = left_data[['BMIä¸­å¿ƒ', 'å¹³å‡æµ“åº¦']].corr().iloc[0,1]
                print(f"å³°å€¼å·¦ä¾§ç›¸å…³ç³»æ•°: {correlation_left:.3f} (åº”ä¸ºæ­£å€¼)")
            
            if len(right_data) >= 3:
                correlation_right = right_data[['BMIä¸­å¿ƒ', 'å¹³å‡æµ“åº¦']].corr().iloc[0,1]
                print(f"å³°å€¼å³ä¾§ç›¸å…³ç³»æ•°: {correlation_right:.3f} (åº”ä¸ºè´Ÿå€¼)")
            
            # åˆ¤æ–­æ˜¯å¦å­˜åœ¨å€’Uå‹å…³ç³»
            has_inverted_u = False
            if correlation_left is not None and correlation_right is not None:
                if correlation_left > 0.1 and correlation_right < -0.1:
                    has_inverted_u = True
                    print(f"âœ… éªŒè¯å‘ç°å€’Uå‹å…³ç³»ï¼")
                else:
                    print(f"âŒ æœªå‘ç°æ˜æ˜¾çš„å€’Uå‹å…³ç³»")
            
            self.peak_bmi = peak_bmi
            self.peak_concentration = peak_concentration
            self.has_inverted_u = has_inverted_u
            
            return True
        else:
            print("âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æBMI-æµ“åº¦å…³ç³»")
            return False
    
    def analyze_bmi_timing_relationship(self):
        """åˆ†æBMIä¸è¾¾æ ‡æ—¶é—´çš„å…³ç³»"""
        print("\n" + "="*60)
        print("æ­¥éª¤2: åˆ†æBMIä¸è¾¾æ ‡æ—¶é—´çš„å…³ç³»")
        print("="*60)
        
        # åªåˆ†æè§‚å¯Ÿåˆ°è¾¾æ ‡äº‹ä»¶çš„æ•°æ®
        observed_data = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1].copy()
        print(f"åˆ†ææ ·æœ¬æ•°: {len(observed_data)} (å·²è¾¾æ ‡çš„å­•å¦‡)")
        
        if len(observed_data) < 10:
            print("âŒ è¾¾æ ‡æ ·æœ¬æ•°ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯é åˆ†æ")
            return False
        
        # 1. åˆ†æBMIåŒºé—´ä¸è¾¾æ ‡æ—¶é—´çš„å…³ç³»
        bmi_timing_stats = []
        bmi_ranges = [(25, 28), (28, 30), (30, 32), (32, 34), (34, 36), (36, 40), (40, 50)]
        
        for bmi_min, bmi_max in bmi_ranges:
            bmi_data = observed_data[(observed_data['BMI'] >= bmi_min) & 
                                   (observed_data['BMI'] < bmi_max)]
            if len(bmi_data) >= 3:  # è‡³å°‘3ä¸ªæ ·æœ¬
                stats_dict = {
                    'BMIåŒºé—´': f'[{bmi_min}, {bmi_max})',
                    'BMIä¸­å¿ƒ': (bmi_min + bmi_max) / 2,
                    'æ ·æœ¬æ•°': len(bmi_data),
                    'å¹³å‡è¾¾æ ‡æ—¶é—´': bmi_data['äº‹ä»¶æ—¶é—´'].mean(),
                    'è¾¾æ ‡æ—¶é—´ä¸­ä½æ•°': bmi_data['äº‹ä»¶æ—¶é—´'].median(),
                    'è¾¾æ ‡æ—¶é—´æ ‡å‡†å·®': bmi_data['äº‹ä»¶æ—¶é—´'].std(),
                    '25%åˆ†ä½æ•°': bmi_data['äº‹ä»¶æ—¶é—´'].quantile(0.25),
                    '75%åˆ†ä½æ•°': bmi_data['äº‹ä»¶æ—¶é—´'].quantile(0.75)
                }
                bmi_timing_stats.append(stats_dict)
                print(f"BMI {bmi_min}-{bmi_max}: æ ·æœ¬æ•°={len(bmi_data)}, "
                      f"å¹³å‡è¾¾æ ‡æ—¶é—´={stats_dict['å¹³å‡è¾¾æ ‡æ—¶é—´']:.1f}å‘¨")
        
        self.bmi_timing_stats = pd.DataFrame(bmi_timing_stats)
        
        # 2. æ‰¾åˆ°è¾¾æ ‡æ—¶é—´æœ€çŸ­çš„BMIåŒºé—´
        if len(self.bmi_timing_stats) > 0:
            min_timing_idx = self.bmi_timing_stats['å¹³å‡è¾¾æ ‡æ—¶é—´'].idxmin()
            optimal_bmi_range = self.bmi_timing_stats.loc[min_timing_idx, 'BMIåŒºé—´']
            optimal_timing = self.bmi_timing_stats.loc[min_timing_idx, 'å¹³å‡è¾¾æ ‡æ—¶é—´']
            
            print(f"\nå…³é”®å‘ç°:")
            print(f"è¾¾æ ‡æ—¶é—´æœ€çŸ­çš„BMIåŒºé—´: {optimal_bmi_range}")
            print(f"æœ€çŸ­å¹³å‡è¾¾æ ‡æ—¶é—´: {optimal_timing:.1f}å‘¨")
            
            self.optimal_bmi_range = optimal_bmi_range
            self.optimal_timing = optimal_timing
            
            return True
        else:
            print("âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æBMI-è¾¾æ ‡æ—¶é—´å…³ç³»")
            return False
    
    def data_driven_bmi_grouping(self):
        """åŸºäºæ•°æ®é©±åŠ¨çš„BMIåˆ†ç»„æ–¹æ³•"""
        print("\n" + "="*60)
        print("æ­¥éª¤3: æ•°æ®é©±åŠ¨çš„BMIåˆ†ç»„")
        print("="*60)
        
        observed_data = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1].copy()
        
        if len(observed_data) < 20:
            print("âŒ æ ·æœ¬æ•°ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç»„")
            return self.fallback_grouping()
        
        # æ–¹æ³•1: åŸºäºå†³ç­–æ ‘çš„åˆ†ç»„
        print("æ–¹æ³•1: åŸºäºå†³ç­–æ ‘çš„æœ€ä¼˜åˆ†ç»„")
        
        # å‡†å¤‡æ•°æ®
        X = observed_data[['BMI']].values
        y = observed_data['äº‹ä»¶æ—¶é—´'].values
        
        # ç½‘æ ¼æœç´¢æœ€ä¼˜å†³ç­–æ ‘å‚æ•°
        param_grid = {
            'max_depth': [2, 3, 4, 5],
            'min_samples_split': [10, 15, 20],
            'min_samples_leaf': [5, 8, 10]
        }
        
        tree = DecisionTreeRegressor(random_state=42)
        grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='neg_mean_squared_error')
        grid_search.fit(X, y)
        
        best_tree = grid_search.best_estimator_
        print(f"æœ€ä¼˜å†³ç­–æ ‘å‚æ•°: {grid_search.best_params_}")
        print(f"äº¤å‰éªŒè¯å¾—åˆ†: {-grid_search.best_score_:.3f}")
        
        # è·å–åˆ†å‰²ç‚¹
        tree_splits = self.extract_tree_splits(best_tree, X, y)
        print(f"å†³ç­–æ ‘åˆ†å‰²ç‚¹: {tree_splits}")
        
        # æ–¹æ³•2: åŸºäºåˆ†ä½æ•°çš„åˆ†ç»„
        print("\næ–¹æ³•2: åŸºäºåˆ†ä½æ•°çš„åˆ†ç»„")
        bmi_quartiles = observed_data['BMI'].quantile([0.25, 0.5, 0.75]).values
        quartile_splits = [25] + list(bmi_quartiles) + [50]  # æ·»åŠ è¾¹ç•Œå€¼
        quartile_splits = sorted(list(set(quartile_splits)))  # å»é‡å¹¶æ’åº
        print(f"åˆ†ä½æ•°åˆ†å‰²ç‚¹: {quartile_splits}")
        
        # æ–¹æ³•3: åŸºäºK-meansèšç±»çš„åˆ†ç»„
        print("\næ–¹æ³•3: åŸºäºK-meansèšç±»çš„åˆ†ç»„")
        best_k = self.find_optimal_clusters(observed_data)
        
        if best_k > 1:
            kmeans = KMeans(n_clusters=best_k, random_state=42)
            clusters = kmeans.fit_predict(observed_data[['BMI', 'äº‹ä»¶æ—¶é—´']])
            cluster_centers = kmeans.cluster_centers_
            
            # æŒ‰BMIæ’åºèšç±»ä¸­å¿ƒ
            sorted_centers = sorted(cluster_centers, key=lambda x: x[0])
            cluster_splits = []
            for i in range(len(sorted_centers)-1):
                split_point = (sorted_centers[i][0] + sorted_centers[i+1][0]) / 2
                cluster_splits.append(split_point)
            
            print(f"K-meansåˆ†å‰²ç‚¹ (k={best_k}): {cluster_splits}")
        else:
            cluster_splits = []
        
        # ç»¼åˆè¯„ä¼°ä¸‰ç§æ–¹æ³•
        print("\næ–¹æ³•è¯„ä¼°ä¸é€‰æ‹©:")
        all_methods = [
            ('å†³ç­–æ ‘', tree_splits),
            ('åˆ†ä½æ•°', quartile_splits[1:-1]),  # å»æ‰è¾¹ç•Œå€¼
            ('K-means', cluster_splits)
        ]
        
        best_method = None
        best_score = float('inf')
        
        for method_name, splits in all_methods:
            if len(splits) >= 2:  # è‡³å°‘è¦æœ‰2ä¸ªåˆ†å‰²ç‚¹ï¼ˆ3ç»„ï¼‰
                score = self.evaluate_grouping_quality(observed_data, splits)
                print(f"{method_name}åˆ†ç»„è´¨é‡è¯„åˆ†: {score:.3f}")
                
                if score < best_score:
                    best_score = score
                    best_method = (method_name, splits)
        
        if best_method:
            method_name, final_splits = best_method
            print(f"\nâœ… é€‰æ‹©æœ€ä¼˜æ–¹æ³•: {method_name}")
            print(f"æœ€ä¼˜åˆ†å‰²ç‚¹: {final_splits}")
            
            # åˆ›å»ºåˆ†ç»„
            self.create_data_driven_groups(final_splits)
            return True
        else:
            print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½ä¸é€‚ç”¨ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç»„")
            return self.fallback_grouping()
    
    def extract_tree_splits(self, tree, X, y):
        """ä»å†³ç­–æ ‘ä¸­æå–åˆ†å‰²ç‚¹"""
        tree_ = tree.tree_
        splits = []
        
        def recurse(node):
            if tree_.feature[node] != -2:  # ä¸æ˜¯å¶å­èŠ‚ç‚¹
                threshold = tree_.threshold[node]
                splits.append(threshold)
                recurse(tree_.children_left[node])
                recurse(tree_.children_right[node])
        
        recurse(0)
        return sorted(list(set(splits)))
    
    def find_optimal_clusters(self, data):
        """å¯»æ‰¾æœ€ä¼˜èšç±»æ•°"""
        X = data[['BMI', 'äº‹ä»¶æ—¶é—´']].values
        
        silhouette_scores = []
        k_range = range(2, min(8, len(data)//3))  # æœ€å¤š7ç»„ï¼Œæ¯ç»„è‡³å°‘3ä¸ªæ ·æœ¬
        
        for k in k_range:
            if k <= len(data):
                kmeans = KMeans(n_clusters=k, random_state=42)
                labels = kmeans.fit_predict(X)
                score = silhouette_score(X, labels)
                silhouette_scores.append(score)
                print(f"K={k}, è½®å»“ç³»æ•°={score:.3f}")
        
        if silhouette_scores:
            best_k = k_range[np.argmax(silhouette_scores)]
            print(f"æœ€ä¼˜èšç±»æ•°: K={best_k}")
            return best_k
        else:
            return 0
    
    def evaluate_grouping_quality(self, data, splits):
        """è¯„ä¼°åˆ†ç»„è´¨é‡"""
        try:
            # åˆ›å»ºåˆ†ç»„
            groups = self.assign_groups_by_splits(data['BMI'], splits)
            
            if len(set(groups)) < 2:
                return float('inf')
            
            # è®¡ç®—ç»„å†…æ–¹å·®å’Œç»„é—´æ–¹å·®æ¯”
            group_variances = []
            group_means = []
            
            for group_id in set(groups):
                group_data = data[groups == group_id]['äº‹ä»¶æ—¶é—´']
                if len(group_data) > 1:
                    group_variances.append(group_data.var())
                    group_means.append(group_data.mean())
            
            if len(group_variances) < 2:
                return float('inf')
            
            within_group_var = np.mean(group_variances)
            between_group_var = np.var(group_means)
            
            # è¯„åˆ†ï¼šç»„å†…æ–¹å·®è¶Šå°ï¼Œç»„é—´æ–¹å·®è¶Šå¤§ï¼Œè¯„åˆ†è¶Šä½ï¼ˆè¶Šå¥½ï¼‰
            score = within_group_var / (between_group_var + 1e-6)
            
            return score
            
        except:
            return float('inf')
    
    def assign_groups_by_splits(self, bmi_values, splits):
        """æ ¹æ®åˆ†å‰²ç‚¹åˆ†é…ç»„åˆ«"""
        groups = np.zeros(len(bmi_values))
        
        for i, bmi in enumerate(bmi_values):
            group_id = 1
            for split in sorted(splits):
                if bmi >= split:
                    group_id += 1
                else:
                    break
            groups[i] = group_id
        
        return groups
    
    def create_data_driven_groups(self, splits):
        """æ ¹æ®æ•°æ®é©±åŠ¨çš„åˆ†å‰²ç‚¹åˆ›å»ºåˆ†ç»„"""
        # æ·»åŠ è¾¹ç•Œå€¼
        full_splits = [0] + sorted(splits) + [100]
        
        def assign_group(bmi):
            if pd.isna(bmi):
                return 0
            
            for i, split in enumerate(full_splits[:-1]):
                if bmi >= split and bmi < full_splits[i+1]:
                    return i + 1
            return len(full_splits) - 1
        
        self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'] = self.survival_df['BMI'].apply(assign_group)
        
        # æ˜¾ç¤ºåˆ†ç»„ç»“æœ
        print(f"\næ•°æ®é©±åŠ¨çš„BMIåˆ†ç»„ç»“æœ:")
        for group_id in sorted(self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'].unique()):
            if group_id == 0:
                continue
            
            group_data = self.survival_df[self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(group_data) > 0:
                bmi_min = group_data['BMI'].min()
                bmi_max = group_data['BMI'].max()
                avg_timing = observed_data['äº‹ä»¶æ—¶é—´'].mean() if len(observed_data) > 0 else np.nan
                reach_rate = observed_data.shape[0] / group_data.shape[0]
                
                print(f"ç»„{group_id}: BMI[{bmi_min:.1f}, {bmi_max:.1f}], "
                      f"æ ·æœ¬æ•°={len(group_data)}, è¾¾æ ‡ç‡={reach_rate:.1%}, "
                      f"å¹³å‡è¾¾æ ‡æ—¶é—´={avg_timing:.1f}å‘¨")
        
        # è®¡ç®—åŸºäºæ•°æ®çš„é£é™©å› å­
        self.calculate_data_driven_risk_factors()
        
        return True
    
    def calculate_data_driven_risk_factors(self):
        """åŸºäºçœŸå®æ•°æ®è®¡ç®—é£é™©å› å­"""
        print(f"\nè®¡ç®—æ•°æ®é©±åŠ¨çš„é£é™©å› å­:")
        
        # è®¡ç®—å„ç»„çš„å®é™…é£é™©æŒ‡æ ‡
        group_risk_factors = {}
        
        for group_id in sorted(self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'].unique()):
            if group_id == 0:
                continue
                
            group_data = self.survival_df[self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(group_data) > 0:
                # é£é™©å› å­åŸºäºï¼š1-è¾¾æ ‡ç‡ + æ ‡å‡†åŒ–è¾¾æ ‡æ—¶é—´
                reach_rate = len(observed_data) / len(group_data)
                avg_timing = observed_data['äº‹ä»¶æ—¶é—´'].mean() if len(observed_data) > 0 else 20.0
                
                # æ ‡å‡†åŒ–è¾¾æ ‡æ—¶é—´ï¼ˆç›¸å¯¹äºå…¨ä½“å¹³å‡ï¼‰
                overall_avg_timing = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1]['äº‹ä»¶æ—¶é—´'].mean()
                timing_risk = (avg_timing - overall_avg_timing) / overall_avg_timing
                
                # ç»¼åˆé£é™©å› å­
                risk_factor = (1 - reach_rate) * 2 + max(0, timing_risk) * 1.5 + 0.5
                risk_factor = max(0.3, min(2.0, risk_factor))  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
                
                group_risk_factors[group_id] = risk_factor
                
                print(f"ç»„{group_id}: è¾¾æ ‡ç‡={reach_rate:.1%}, å¹³å‡è¾¾æ ‡æ—¶é—´={avg_timing:.1f}å‘¨, "
                      f"é£é™©å› å­={risk_factor:.2f}")
        
        self.data_driven_risk_factors = group_risk_factors
        
        # æ›´æ–°survival_dfä¸­çš„é£é™©å› å­
        def get_data_driven_risk_factor(bmi):
            if pd.isna(bmi):
                return 1.0
            
            # æ‰¾åˆ°å¯¹åº”çš„ç»„
            for group_id in sorted(self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'].unique()):
                if group_id == 0:
                    continue
                group_data = self.survival_df[self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'] == group_id]
                if len(group_data) > 0:
                    bmi_min = group_data['BMI'].min()
                    bmi_max = group_data['BMI'].max()
                    if bmi_min <= bmi <= bmi_max:
                        return self.data_driven_risk_factors.get(group_id, 1.0)
            
            return 1.0  # é»˜è®¤å€¼
        
        self.survival_df['æ•°æ®é©±åŠ¨é£é™©å› å­'] = self.survival_df['BMI'].apply(get_data_driven_risk_factor)
    
    def fallback_grouping(self):
        """å¤‡ç”¨åˆ†ç»„æ–¹æ³•ï¼ˆå½“æ•°æ®ä¸è¶³æ—¶ï¼‰"""
        print("ä½¿ç”¨å¤‡ç”¨åˆ†ç»„æ–¹æ³•ï¼ˆåŸºäºåŒ»å­¦æ ‡å‡†ï¼‰")
        
        def assign_fallback_group(bmi):
            if pd.isna(bmi):
                return 0
            elif bmi < 28:
                return 1
            elif bmi < 32:
                return 2
            elif bmi < 36:
                return 3
            else:
                return 4
        
        self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'] = self.survival_df['BMI'].apply(assign_fallback_group)
        
        # ç®€å•çš„é£é™©å› å­
        self.data_driven_risk_factors = {1: 1.2, 2: 0.9, 3: 1.1, 4: 1.4}
        self.survival_df['æ•°æ®é©±åŠ¨é£é™©å› å­'] = self.survival_df['BMI'].apply(
            lambda bmi: self.data_driven_risk_factors.get(assign_fallback_group(bmi), 1.0)
        )
        
        return True
    
    def calculate_optimal_timepoints(self):
        """è®¡ç®—å„ç»„æœ€ä½³æ£€æµ‹æ—¶ç‚¹"""
        print("\n" + "="*60)
        print("æ­¥éª¤4: è®¡ç®—æ•°æ®é©±åŠ¨çš„æœ€ä½³æ£€æµ‹æ—¶ç‚¹")
        print("="*60)
        
        recommendations = []
        
        for group_id in sorted(self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'].unique()):
            if group_id == 0:
                continue
                
            group_data = self.survival_df[self.survival_df['æ•°æ®é©±åŠ¨BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(observed_data) < 2:
                continue
            
            # è·å–ç»„ä¿¡æ¯
            bmi_min = group_data['BMI'].min()
            bmi_max = group_data['BMI'].max()
            group_name = f'BMI[{bmi_min:.1f},{bmi_max:.1f})ç»„'
            
            # è®¡ç®—åˆ†ä½æ•°æ—¶ç‚¹
            times = observed_data['äº‹ä»¶æ—¶é—´'].values
            percentile_80 = np.percentile(times, 80)
            percentile_90 = np.percentile(times, 90)
            
            # é£é™©ä¼˜åŒ–æ—¶ç‚¹
            risk_factor = self.data_driven_risk_factors.get(group_id, 1.0)
            
            def risk_function(t):
                early_risk = max(0.1, np.mean(times > t))
                
                if t <= 12:
                    delay_risk = 0.05
                elif t <= 16:
                    delay_risk = 0.15
                elif t <= 20:
                    delay_risk = 0.35
                elif t <= 24:
                    delay_risk = 0.65
                else:
                    delay_risk = 0.90
                
                total_risk = 0.3 * early_risk * risk_factor + 0.7 * delay_risk
                return total_risk
            
            from scipy.optimize import minimize_scalar
            result = minimize_scalar(risk_function, bounds=(11, 22), method='bounded')
            optimal_time = result.x
            
            # æœ€ç»ˆæ¨è
            final_recommendation = min(
                (percentile_80 + optimal_time) / 2,
                20.0
            )
            final_recommendation = max(12.0, final_recommendation)
            
            recommendation = {
                'BMIç»„': group_id,
                'ç»„å': group_name,
                'BMIåŒºé—´': f"[{bmi_min:.1f}, {bmi_max:.1f}]",
                'æ€»æ ·æœ¬æ•°': len(group_data),
                'è¾¾æ ‡æ ·æœ¬æ•°': len(observed_data),
                'è¾¾æ ‡ç‡': f"{len(observed_data)/len(group_data):.1%}",
                'å¹³å‡è¾¾æ ‡æ—¶é—´': f"{observed_data['äº‹ä»¶æ—¶é—´'].mean():.1f}å‘¨",
                '80%åˆ†ä½æ•°æ—¶ç‚¹': f"{percentile_80:.1f}å‘¨",
                '90%åˆ†ä½æ•°æ—¶ç‚¹': f"{percentile_90:.1f}å‘¨",
                'é£é™©æœ€ä¼˜æ—¶ç‚¹': f"{optimal_time:.1f}å‘¨",
                'æ•°æ®é©±åŠ¨é£é™©å› å­': f"{risk_factor:.2f}",
                'æ¨èæ£€æµ‹æ—¶ç‚¹': f"{final_recommendation:.1f}å‘¨",
                'æ–¹æ³•': 'æ•°æ®é©±åŠ¨'
            }
            
            recommendations.append(recommendation)
            
            print(f"\n{group_name}:")
            print(f"  æ ·æœ¬ç‰¹å¾: æ€»æ•°{len(group_data)}, è¾¾æ ‡{len(observed_data)}")
            print(f"  æ•°æ®é©±åŠ¨é£é™©å› å­: {risk_factor:.2f}")
            print(f"  ğŸ¯ æ¨èæ£€æµ‹æ—¶ç‚¹: {final_recommendation:.1f}å‘¨")
        
        self.recommendations_df = pd.DataFrame(recommendations)
        return self.recommendations_df
    
    def create_comparison_visualization(self):
        """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # å·¦ä¸Šï¼šBMIä¸æµ“åº¦å…³ç³»
        if hasattr(self, 'bmi_concentration_stats') and len(self.bmi_concentration_stats) > 0:
            ax1.scatter(self.bmi_concentration_stats['BMIä¸­å¿ƒ'], 
                       self.bmi_concentration_stats['å¹³å‡æµ“åº¦'],
                       s=self.bmi_concentration_stats['æ ·æœ¬æ•°']*2,
                       alpha=0.7, c='blue')
            
            # æ‹Ÿåˆæ›²çº¿
            from scipy.optimize import curve_fit
            def quadratic(x, a, b, c):
                return a * x**2 + b * x + c
            
            try:
                popt, _ = curve_fit(quadratic, 
                                  self.bmi_concentration_stats['BMIä¸­å¿ƒ'], 
                                  self.bmi_concentration_stats['å¹³å‡æµ“åº¦'])
                x_smooth = np.linspace(self.bmi_concentration_stats['BMIä¸­å¿ƒ'].min(),
                                     self.bmi_concentration_stats['BMIä¸­å¿ƒ'].max(), 100)
                y_smooth = quadratic(x_smooth, *popt)
                ax1.plot(x_smooth, y_smooth, 'r-', linewidth=2, label='äºŒæ¬¡æ‹Ÿåˆ')
                ax1.legend()
            except:
                pass
            
            ax1.set_xlabel('BMI', fontweight='bold')
            ax1.set_ylabel('å¹³å‡YæŸ“è‰²ä½“æµ“åº¦', fontweight='bold')
            ax1.set_title('BMIä¸YæŸ“è‰²ä½“æµ“åº¦å…³ç³»ï¼ˆæ•°æ®é©±åŠ¨å‘ç°ï¼‰', fontweight='bold')
            ax1.grid(True, alpha=0.3)
        
        # å³ä¸Šï¼šBMIä¸è¾¾æ ‡æ—¶é—´å…³ç³»
        if hasattr(self, 'bmi_timing_stats') and len(self.bmi_timing_stats) > 0:
            ax2.scatter(self.bmi_timing_stats['BMIä¸­å¿ƒ'], 
                       self.bmi_timing_stats['å¹³å‡è¾¾æ ‡æ—¶é—´'],
                       s=self.bmi_timing_stats['æ ·æœ¬æ•°']*3,
                       alpha=0.7, c='green')
            
            ax2.set_xlabel('BMI', fontweight='bold')
            ax2.set_ylabel('å¹³å‡è¾¾æ ‡æ—¶é—´(å‘¨)', fontweight='bold')
            ax2.set_title('BMIä¸è¾¾æ ‡æ—¶é—´å…³ç³»ï¼ˆæ•°æ®é©±åŠ¨å‘ç°ï¼‰', fontweight='bold')
            ax2.grid(True, alpha=0.3)
        
        # å·¦ä¸‹ï¼šæ•°æ®é©±åŠ¨åˆ†ç»„ç»“æœ
        if hasattr(self, 'recommendations_df'):
            groups = self.recommendations_df['BMIç»„'].values
            timepoints = [float(x.split('å‘¨')[0]) for x in self.recommendations_df['æ¨èæ£€æµ‹æ—¶ç‚¹']]
            risk_factors = [float(x) for x in self.recommendations_df['æ•°æ®é©±åŠ¨é£é™©å› å­']]
            
            bars = ax3.bar(groups, timepoints, alpha=0.7, 
                          color=plt.cm.viridis(np.linspace(0, 1, len(groups))))
            
            for bar, risk in zip(bars, risk_factors):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.2,
                        f'{height:.1f}å‘¨\n(é£é™©{risk:.2f})', 
                        ha='center', va='bottom', fontweight='bold', fontsize=9)
            
            ax3.set_xlabel('æ•°æ®é©±åŠ¨BMIç»„', fontweight='bold')
            ax3.set_ylabel('æ¨èæ£€æµ‹æ—¶ç‚¹(å‘¨)', fontweight='bold')
            ax3.set_title('æ•°æ®é©±åŠ¨çš„æ£€æµ‹æ—¶ç‚¹æ¨è', fontweight='bold')
            ax3.grid(True, alpha=0.3)
        
        # å³ä¸‹ï¼šé£é™©å› å­å¯¹æ¯”
        if hasattr(self, 'data_driven_risk_factors'):
            groups = list(self.data_driven_risk_factors.keys())
            risk_factors = list(self.data_driven_risk_factors.values())
            
            colors = ['green' if rf < 1.0 else 'orange' if rf < 1.3 else 'red' for rf in risk_factors]
            bars = ax4.bar(groups, risk_factors, alpha=0.7, color=colors)
            
            for bar, rf in zip(bars, risk_factors):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                        f'{rf:.2f}', ha='center', va='bottom', fontweight='bold')
            
            ax4.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='åŸºå‡†é£é™©')
            ax4.set_xlabel('æ•°æ®é©±åŠ¨BMIç»„', fontweight='bold')
            ax4.set_ylabel('æ•°æ®é©±åŠ¨é£é™©å› å­', fontweight='bold')
            ax4.set_title('å„ç»„æ•°æ®é©±åŠ¨é£é™©å› å­', fontweight='bold')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('data_driven_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_data_driven_report(self):
        """ç”Ÿæˆæ•°æ®é©±åŠ¨åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("æ•°æ®é©±åŠ¨çš„NIPTä¼˜åŒ–æ–¹æ¡ˆ - åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        print("\nğŸ“Š æ•°æ®é©±åŠ¨å‘ç°:")
        
        # BMI-æµ“åº¦å…³ç³»å‘ç°
        if hasattr(self, 'peak_bmi'):
            print(f"âœ… YæŸ“è‰²ä½“æµ“åº¦å³°å€¼BMI: {self.peak_bmi:.1f}")
            print(f"âœ… å³°å€¼æµ“åº¦: {self.peak_concentration:.4f}")
            if hasattr(self, 'has_inverted_u') and self.has_inverted_u:
                print(f"âœ… éªŒè¯å‘ç°å€’Uå‹å…³ç³»")
            else:
                print(f"âš ï¸  æœªå‘ç°æ˜æ˜¾å€’Uå‹å…³ç³»")
        
        # BMI-è¾¾æ ‡æ—¶é—´å…³ç³»å‘ç°
        if hasattr(self, 'optimal_bmi_range'):
            print(f"âœ… è¾¾æ ‡æœ€å¿«BMIåŒºé—´: {self.optimal_bmi_range}")
            print(f"âœ… æœ€å¿«è¾¾æ ‡æ—¶é—´: {self.optimal_timing:.1f}å‘¨")
        
        print(f"\nğŸ¯ æ•°æ®é©±åŠ¨çš„BMIåˆ†ç»„ä¸æ¨èæ—¶ç‚¹:")
        print("-" * 80)
        
        if hasattr(self, 'recommendations_df'):
            for _, rec in self.recommendations_df.iterrows():
                print(f"\nğŸ“‹ {rec['ç»„å']}:")
                print(f"   â€¢ æ ·æœ¬æ„æˆ: æ€»æ•°{rec['æ€»æ ·æœ¬æ•°']}äºº, è¾¾æ ‡{rec['è¾¾æ ‡æ ·æœ¬æ•°']}äºº")
                print(f"   â€¢ è¾¾æ ‡ç‡: {rec['è¾¾æ ‡ç‡']}")
                print(f"   â€¢ æ•°æ®é©±åŠ¨é£é™©å› å­: {rec['æ•°æ®é©±åŠ¨é£é™©å› å­']}")
                print(f"   â€¢ ğŸ¯ æ¨èæ£€æµ‹æ—¶ç‚¹: {rec['æ¨èæ£€æµ‹æ—¶ç‚¹']}")
                print(f"   â€¢ æ–¹æ³•: {rec['æ–¹æ³•']}")
        
        print(f"\nğŸ’¡ æ–¹æ³•å­¦ä¼˜åŠ¿:")
        print("1. å®Œå…¨åŸºäºçœŸå®æ•°æ®ï¼Œé¿å…ä¸»è§‚å‡è®¾")
        print("2. ä½¿ç”¨å¤šç§åˆ†ç»„æ–¹æ³•ï¼ˆå†³ç­–æ ‘ã€åˆ†ä½æ•°ã€èšç±»ï¼‰å¹¶é€‰æ‹©æœ€ä¼˜")
        print("3. é£é™©å› å­åŸºäºå®é™…è¾¾æ ‡ç‡å’Œè¾¾æ ‡æ—¶é—´è®¡ç®—")
        print("4. ç»è¿‡äº¤å‰éªŒè¯å’Œè´¨é‡è¯„ä¼°")
        
        print(f"\nğŸ”¬ ç§‘å­¦æ€§éªŒè¯:")
        print("âœ… æ•°æ®é©±åŠ¨çš„åˆ†ç»„è¾¹ç•Œ")
        print("âœ… åŸºäºçœŸå®è¾¾æ ‡æ•°æ®çš„é£é™©è¯„ä¼°")
        print("âœ… å¤šæ–¹æ³•äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ")
        print("âœ… é¿å…é¢„è®¾ç»“è®ºçš„æ–¹æ³•å­¦åè¯¯")
        
        # ä¿å­˜ç»“æœ
        if hasattr(self, 'recommendations_df'):
            self.recommendations_df.to_excel('data_driven_recommendations.xlsx', index=False)
            print(f"\nâœ… æ•°æ®é©±åŠ¨æ¨èæ–¹æ¡ˆå·²ä¿å­˜è‡³: data_driven_recommendations.xlsx")
        
        return True
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é©±åŠ¨åˆ†æ"""
        print("NIPTé—®é¢˜2: æ•°æ®é©±åŠ¨çš„BMIåˆ†ç»„ä¸æ£€æµ‹æ—¶ç‚¹ä¼˜åŒ–")
        print("="*80)
        
        # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        if not self.load_and_process_data():
            return False
        
        # 2. åˆ†æBMIä¸æµ“åº¦å…³ç³»
        if not self.analyze_bmi_concentration_relationship():
            print("âš ï¸ BMI-æµ“åº¦å…³ç³»åˆ†æå¤±è´¥ï¼Œç»§ç»­å…¶ä»–åˆ†æ")
        
        # 3. åˆ†æBMIä¸è¾¾æ ‡æ—¶é—´å…³ç³»
        if not self.analyze_bmi_timing_relationship():
            print("âš ï¸ BMI-è¾¾æ ‡æ—¶é—´å…³ç³»åˆ†æå¤±è´¥ï¼Œç»§ç»­å…¶ä»–åˆ†æ")
        
        # 4. æ•°æ®é©±åŠ¨BMIåˆ†ç»„
        if not self.data_driven_bmi_grouping():
            return False
        
        # 5. è®¡ç®—æœ€ä½³æ£€æµ‹æ—¶ç‚¹
        self.calculate_optimal_timepoints()
        
        # 6. åˆ›å»ºå¯è§†åŒ–
        self.create_comparison_visualization()
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        self.generate_data_driven_report()
        
        return True

# ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    analyzer = DataDrivenNIPTOptimizer()
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\n" + "="*80)
        print("ğŸ‰ æ•°æ®é©±åŠ¨çš„NIPTä¼˜åŒ–åˆ†æå®Œæˆï¼")
        print("="*80)
        print("æ ¸å¿ƒæ”¹è¿›:")
        print("âœ… å®Œå…¨åŸºäºçœŸå®æ•°æ®çš„BMIåˆ†ç»„")
        print("âœ… æ•°æ®é©±åŠ¨çš„é£é™©å› å­è®¡ç®—") 
        print("âœ… å¤šæ–¹æ³•éªŒè¯é€‰æ‹©æœ€ä¼˜åˆ†ç»„")
        print("âœ… é¿å…é¢„è®¾ç»“è®ºçš„ç§‘å­¦æ–¹æ³•")
    else:
        print("âŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
