# NIPTé—®é¢˜2ï¼šåŸºäºç”Ÿå­˜åˆ†æçš„BMIåˆ†ç»„ä¸æœ€ä½³æ£€æµ‹æ—¶ç‚¹ä¼˜åŒ–
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.optimize import minimize_scalar
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (15, 10)

class SurvivalBasedNIPTOptimizer:
    """åŸºäºç”Ÿå­˜åˆ†ææ€æƒ³çš„NIPTä¼˜åŒ–å™¨"""
    
    def __init__(self, data_file='é™„ä»¶.xlsx'):
        self.data_file = data_file
        self.threshold = 0.04
        self.alpha = 0.1  # 90%ç½®ä¿¡æ°´å¹³
        
    def load_and_process_data(self):
        """åŠ è½½å¹¶å¤„ç†æ•°æ®ï¼Œè€ƒè™‘åˆ å¤±æƒ…å†µ"""
        print("="*80)
        print("æ•°æ®åŠ è½½ä¸ç”Ÿå­˜åˆ†æé¢„å¤„ç†")
        print("="*80)
        
        try:
            # è¯»å–åŸå§‹æ•°æ®
            original_data = pd.read_excel(self.data_file, sheet_name='ç”·èƒæ£€æµ‹æ•°æ®')
            
            # è½¬æ¢å­•å‘¨æ ¼å¼
            def convert_gestation_week(week_str):
                if pd.isna(week_str):
                    return np.nan
                week_str = str(week_str).strip().upper()
                
                if 'W' in week_str:
                    week_str = week_str.replace('W', 'w')
                
                if 'w' in week_str.lower():
                    parts = week_str.lower().split('w')
                    try:
                        weeks = int(parts[0])
                        if len(parts) > 1 and '+' in parts[1]:
                            days = int(parts[1].split('+')[1])
                            return weeks + days/7
                        else:
                            return float(weeks)
                    except ValueError:
                        return np.nan
                
                try:
                    return float(week_str)
                except ValueError:
                    return np.nan
            
            original_data['å­•å‘¨_æ•°å€¼'] = original_data['æ£€æµ‹å­•å‘¨'].apply(convert_gestation_week)
            
            # ç”Ÿå­˜åˆ†ææ•°æ®æ„å»º
            survival_data = []
            
            for woman_code in original_data['å­•å¦‡ä»£ç '].unique():
                woman_data = original_data[original_data['å­•å¦‡ä»£ç '] == woman_code].copy()
                woman_data = woman_data.sort_values('å­•å‘¨_æ•°å€¼')
                woman_data = woman_data.dropna(subset=['å­•å‘¨_æ•°å€¼', 'YæŸ“è‰²ä½“æµ“åº¦'])
                
                if len(woman_data) == 0:
                    continue
                
                # åŸºæœ¬ä¿¡æ¯
                bmi = woman_data['å­•å¦‡BMI'].iloc[0]
                age = woman_data['å¹´é¾„'].iloc[0]
                
                # ç”Ÿå­˜åˆ†æå…³é”®ï¼šç¡®å®šäº‹ä»¶æ—¶é—´å’Œåˆ å¤±çŠ¶æ€
                reaching_records = woman_data[woman_data['YæŸ“è‰²ä½“æµ“åº¦'] >= self.threshold]
                
                if len(reaching_records) > 0:
                    # è§‚å¯Ÿåˆ°äº‹ä»¶ï¼ˆè¾¾æ ‡ï¼‰
                    event_time = reaching_records['å­•å‘¨_æ•°å€¼'].iloc[0]
                    censored = 0  # æœªåˆ å¤±
                    event_observed = 1
                else:
                    # å³åˆ å¤±ï¼šæœªè§‚å¯Ÿåˆ°è¾¾æ ‡äº‹ä»¶
                    event_time = woman_data['å­•å‘¨_æ•°å€¼'].max()
                    censored = 1  # å³åˆ å¤±
                    event_observed = 0
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºåŒºé—´åˆ å¤±
                interval_censored = 0
                lower_bound = event_time
                upper_bound = event_time
                
                if event_observed == 1 and len(woman_data) > 1:
                    # æ£€æŸ¥æ˜¯å¦åœ¨ä¸¤æ¬¡æ£€æµ‹ä¹‹é—´è¾¾æ ‡
                    prev_records = woman_data[woman_data['å­•å‘¨_æ•°å€¼'] < event_time]
                    if len(prev_records) > 0:
                        last_below = prev_records.iloc[-1]
                        if last_below['YæŸ“è‰²ä½“æµ“åº¦'] < self.threshold:
                            # åŒºé—´åˆ å¤±ï¼šåœ¨(last_time, event_time]ä¹‹é—´è¾¾æ ‡
                            interval_censored = 1
                            lower_bound = last_below['å­•å‘¨_æ•°å€¼']
                            upper_bound = event_time
                
                survival_data.append({
                    'å­•å¦‡ä»£ç ': woman_code,
                    'BMI': bmi,
                    'å¹´é¾„': age,
                    'äº‹ä»¶æ—¶é—´': event_time,
                    'äº‹ä»¶è§‚å¯Ÿ': event_observed,
                    'å³åˆ å¤±': censored,
                    'åŒºé—´åˆ å¤±': interval_censored,
                    'ä¸‹ç•Œ': lower_bound,
                    'ä¸Šç•Œ': upper_bound,
                    'æœ€å¤§æµ“åº¦': woman_data['YæŸ“è‰²ä½“æµ“åº¦'].max(),
                    'æ£€æµ‹æ¬¡æ•°': len(woman_data)
                })
            
            self.survival_df = pd.DataFrame(survival_data)
            
            print(f"æ€»å­•å¦‡æ•°: {len(self.survival_df)}")
            print(f"è§‚å¯Ÿåˆ°è¾¾æ ‡äº‹ä»¶: {self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].sum()}")
            print(f"å³åˆ å¤±: {self.survival_df['å³åˆ å¤±'].sum()}")
            print(f"åŒºé—´åˆ å¤±: {self.survival_df['åŒºé—´åˆ å¤±'].sum()}")
            print(f"è¾¾æ ‡ç‡: {self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].mean():.1%}")
            
            return True
            
        except Exception as e:
            print(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return False
    
    def kaplan_meier_estimator(self, times, events):
        """ç®€åŒ–çš„Kaplan-Meierä¼°è®¡å™¨"""
        # æ’åº
        sorted_indices = np.argsort(times)
        sorted_times = times[sorted_indices]
        sorted_events = events[sorted_indices]
        
        # è®¡ç®—ç”Ÿå­˜å‡½æ•°
        unique_times = np.unique(sorted_times[sorted_events == 1])
        survival_prob = []
        
        n = len(times)
        for t in unique_times:
            # åœ¨æ—¶é—´tå‘ç”Ÿäº‹ä»¶çš„æ•°é‡
            events_at_t = np.sum((sorted_times == t) & (sorted_events == 1))
            # åœ¨æ—¶é—´tä¹‹å‰çš„é£é™©é›†å¤§å°
            at_risk = np.sum(sorted_times >= t)
            
            if at_risk > 0:
                survival_prob.append(1 - events_at_t / at_risk)
            else:
                survival_prob.append(1.0)
        
        # ç´¯ç§¯ç”Ÿå­˜æ¦‚ç‡
        cumulative_survival = np.cumprod(survival_prob)
        
        return unique_times, 1 - cumulative_survival  # è¿”å›ç´¯ç§¯å‘ç”Ÿæ¦‚ç‡
    
    def optimize_bmi_grouping(self):
        """ä½¿ç”¨å†³ç­–æ ‘ä¼˜åŒ–BMIåˆ†ç»„"""
        print("\n" + "="*60)
        print("åŸºäºå†³ç­–æ ‘çš„BMIæœ€ä¼˜åˆ†ç»„")
        print("="*60)
        
        # å‡†å¤‡æ•°æ®ï¼šåªä½¿ç”¨è§‚å¯Ÿåˆ°äº‹ä»¶çš„æ•°æ®è¿›è¡Œåˆ†ç»„ä¼˜åŒ–
        observed_data = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1].copy()
        
        if len(observed_data) < 10:
            print("è§‚å¯Ÿåˆ°çš„äº‹ä»¶æ•°é‡å¤ªå°‘ï¼Œä½¿ç”¨ä¼ ç»Ÿåˆ†ç»„æ–¹æ³•")
            return self.traditional_grouping()
        
        # ä½¿ç”¨å†³ç­–æ ‘ç¡®å®šæœ€ä¼˜åˆ†ç»„ç‚¹
        X = observed_data[['BMI']].values
        y = observed_data['äº‹ä»¶æ—¶é—´'].values
        
        # å°è¯•ä¸åŒçš„æœ€å¤§å¶å­èŠ‚ç‚¹æ•°
        best_score = -np.inf
        best_n_leaves = 3
        
        for n_leaves in range(3, 8):
            tree = DecisionTreeRegressor(
                max_leaf_nodes=n_leaves,
                min_samples_leaf=max(5, len(observed_data) // 20),
                random_state=42
            )
            scores = cross_val_score(tree, X, y, cv=min(5, len(observed_data) // 10), scoring='neg_mean_squared_error')
            avg_score = scores.mean()
            
            if avg_score > best_score:
                best_score = avg_score
                best_n_leaves = n_leaves
        
        # è®­ç»ƒæœ€ä½³å†³ç­–æ ‘
        best_tree = DecisionTreeRegressor(
            max_leaf_nodes=best_n_leaves,
            min_samples_leaf=max(5, len(observed_data) // 20),
            random_state=42
        )
        best_tree.fit(X, y)
        
        # æå–åˆ†ç»„è¾¹ç•Œ
        leaf_nodes = best_tree.apply(X)
        unique_leaves = np.unique(leaf_nodes)
        
        groups = []
        for leaf in unique_leaves:
            mask = leaf_nodes == leaf
            group_bmis = X[mask, 0]
            group_times = y[mask]
            
            groups.append({
                'BMI_min': group_bmis.min(),
                'BMI_max': group_bmis.max(),
                'BMI_mean': group_bmis.mean(),
                'æ ·æœ¬æ•°': len(group_bmis),
                'å¹³å‡è¾¾æ ‡æ—¶é—´': group_times.mean(),
                'è¾¾æ ‡æ—¶é—´std': group_times.std()
            })
        
        # æŒ‰BMIæ’åº
        groups = sorted(groups, key=lambda x: x['BMI_mean'])
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ç»„å·
        def assign_optimized_group(bmi):
            for i, group in enumerate(groups):
                if group['BMI_min'] <= bmi <= group['BMI_max']:
                    return i + 1
            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œåˆ†é…åˆ°æœ€è¿‘çš„ç»„
            distances = [abs(bmi - group['BMI_mean']) for group in groups]
            return distances.index(min(distances)) + 1
        
        self.survival_df['ä¼˜åŒ–BMIç»„'] = self.survival_df['BMI'].apply(assign_optimized_group)
        
        print("å†³ç­–æ ‘ä¼˜åŒ–åˆ†ç»„ç»“æœ:")
        for i, group in enumerate(groups):
            print(f"ç»„{i+1}: BMI[{group['BMI_min']:.1f}, {group['BMI_max']:.1f}], "
                  f"æ ·æœ¬æ•°={group['æ ·æœ¬æ•°']}, å¹³å‡è¾¾æ ‡æ—¶é—´={group['å¹³å‡è¾¾æ ‡æ—¶é—´']:.1f}å‘¨")
        
        return groups
    
    def traditional_grouping(self):
        """ä¼ ç»Ÿçš„åŒ»å­¦æ ‡å‡†åˆ†ç»„"""
        def assign_traditional_group(bmi):
            if bmi < 25:
                return 1  # æ­£å¸¸
            elif bmi < 28:
                return 2  # è¶…é‡
            elif bmi < 32:
                return 3  # è½»åº¦è‚¥èƒ–
            elif bmi < 36:
                return 4  # ä¸­åº¦è‚¥èƒ–
            else:
                return 5  # é‡åº¦è‚¥èƒ–
        
        self.survival_df['ä¼˜åŒ–BMIç»„'] = self.survival_df['BMI'].apply(assign_traditional_group)
        
        # ç”Ÿæˆåˆ†ç»„ç»Ÿè®¡
        groups = []
        for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(observed_data) > 0:
                groups.append({
                    'BMI_min': group_data['BMI'].min(),
                    'BMI_max': group_data['BMI'].max(),
                    'BMI_mean': group_data['BMI'].mean(),
                    'æ ·æœ¬æ•°': len(observed_data),
                    'å¹³å‡è¾¾æ ‡æ—¶é—´': observed_data['äº‹ä»¶æ—¶é—´'].mean(),
                    'è¾¾æ ‡æ—¶é—´std': observed_data['äº‹ä»¶æ—¶é—´'].std()
                })
        
        return groups
    
    def calculate_optimal_timepoints(self):
        """è®¡ç®—å„ç»„æœ€ä½³æ£€æµ‹æ—¶ç‚¹"""
        print("\n" + "="*60)
        print("è®¡ç®—æœ€ä½³æ£€æµ‹æ—¶ç‚¹ï¼ˆåŸºäºç”Ÿå­˜åˆ†æï¼‰")
        print("="*60)
        
        recommendations = []
        
        for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(observed_data) < 3:
                print(f"ç»„{group_id}æ ·æœ¬é‡è¿‡å°ï¼Œè·³è¿‡")
                continue
            
            # Kaplan-Meierä¼°è®¡
            times = observed_data['äº‹ä»¶æ—¶é—´'].values
            events = np.ones(len(times))  # æ‰€æœ‰è§‚å¯Ÿåˆ°çš„éƒ½æ˜¯äº‹ä»¶
            
            unique_times, cumulative_prob = self.kaplan_meier_estimator(times, events)
            
            # æ‰¾åˆ°80%å’Œ90%åˆ†ä½æ•°å¯¹åº”çš„æ—¶é—´
            target_probs = [0.8, 0.9]
            timepoints = []
            
            for prob in target_probs:
                if len(unique_times) > 0 and len(cumulative_prob) > 0:
                    # çº¿æ€§æ’å€¼æ‰¾åˆ°å¯¹åº”æ—¶é—´ç‚¹
                    if cumulative_prob[-1] >= prob:
                        timepoint = np.interp(prob, cumulative_prob, unique_times)
                    else:
                        # å¦‚æœæœ€å¤§æ¦‚ç‡è¿˜ä¸åˆ°ç›®æ ‡æ¦‚ç‡ï¼Œä½¿ç”¨å¤–æ¨
                        timepoint = unique_times[-1] + (prob - cumulative_prob[-1]) * 2
                else:
                    timepoint = observed_data['äº‹ä»¶æ—¶é—´'].mean()
                
                timepoints.append(timepoint)
            
            # é£é™©æœ€å°åŒ–çš„æœ€ä¼˜æ—¶ç‚¹
            def risk_function(t):
                # æ—©æ£€æµ‹é£é™©ï¼šåœ¨æ—¶é—´tæ—¶æœªè¾¾æ ‡çš„æ¦‚ç‡
                early_risk = np.mean(times > t)
                
                # å»¶è¿Ÿå‘ç°é£é™©
                if t <= 12:
                    delay_risk = 0.1  # æ—©æœŸå‘ç°
                elif t <= 20:
                    delay_risk = 0.3  # ä¸­æœŸå‘ç°
                elif t <= 27:
                    delay_risk = 0.7  # æ™šæœŸå‘ç°
                else:
                    delay_risk = 0.9  # ææ™šæœŸå‘ç°
                
                # ç»¼åˆé£é™©ï¼ˆå¯è°ƒæƒé‡ï¼‰
                total_risk = 0.6 * early_risk + 0.4 * delay_risk
                return total_risk
            
            # ä¼˜åŒ–æ±‚è§£
            result = minimize_scalar(risk_function, bounds=(10, 25), method='bounded')
            optimal_time = result.x
            
            # ç»Ÿè®¡ä¿¡æ¯
            bmi_stats = {
                'min': group_data['BMI'].min(),
                'max': group_data['BMI'].max(),
                'mean': group_data['BMI'].mean()
            }
            
            recommendation = {
                'BMIç»„': group_id,
                'BMIåŒºé—´': f"[{bmi_stats['min']:.1f}, {bmi_stats['max']:.1f}]",
                'æ€»æ ·æœ¬æ•°': len(group_data),
                'è¾¾æ ‡æ ·æœ¬æ•°': len(observed_data),
                'è¾¾æ ‡ç‡': f"{len(observed_data)/len(group_data):.1%}",
                'å¹³å‡BMI': f"{bmi_stats['mean']:.1f}",
                'å¹³å‡è¾¾æ ‡æ—¶é—´': f"{observed_data['äº‹ä»¶æ—¶é—´'].mean():.1f}å‘¨",
                '80%åˆ†ä½æ•°æ—¶ç‚¹': f"{timepoints[0]:.1f}å‘¨",
                '90%åˆ†ä½æ•°æ—¶ç‚¹': f"{timepoints[1]:.1f}å‘¨",
                'é£é™©æœ€ä¼˜æ—¶ç‚¹': f"{optimal_time:.1f}å‘¨",
                'æœ€å°é£é™©å€¼': f"{risk_function(optimal_time):.1%}",
                'æ¨èæ£€æµ‹æ—¶ç‚¹': f"{min(timepoints[0], 20):.1f}å‘¨"  # ä¸è¶…è¿‡20å‘¨
            }
            
            recommendations.append(recommendation)
            
            print(f"\nBMIç»„ {group_id}:")
            print(f"  BMIåŒºé—´: {recommendation['BMIåŒºé—´']}")
            print(f"  è¾¾æ ‡ç‡: {recommendation['è¾¾æ ‡ç‡']}")
            print(f"  é£é™©æœ€ä¼˜æ—¶ç‚¹: {recommendation['é£é™©æœ€ä¼˜æ—¶ç‚¹']}")
            print(f"  æ¨èæ£€æµ‹æ—¶ç‚¹: {recommendation['æ¨èæ£€æµ‹æ—¶ç‚¹']}")
        
        self.recommendations_df = pd.DataFrame(recommendations)
        return self.recommendations_df
    
    def sensitivity_analysis(self):
        """æ•æ„Ÿæ€§åˆ†æï¼šæ£€æµ‹è¯¯å·®å½±å“"""
        print("\n" + "="*60)
        print("æ•æ„Ÿæ€§åˆ†æï¼šæ£€æµ‹è¯¯å·®å½±å“")
        print("="*60)
        
        # YæŸ“è‰²ä½“æµ“åº¦æµ‹é‡è¯¯å·®
        concentration_errors = [0.001, 0.002, 0.005, 0.01]
        
        print("YæŸ“è‰²ä½“æµ“åº¦æµ‹é‡è¯¯å·®æ•æ„Ÿæ€§åˆ†æ:")
        baseline_reaching_rate = self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].mean()
        
        for error in concentration_errors:
            # æ¨¡æ‹Ÿæµ‹é‡è¯¯å·®å½±å“
            simulated_reaching = 0
            total_simulations = 1000
            
            for _ in range(total_simulations):
                # åŠ å…¥éšæœºè¯¯å·®
                noise = np.random.normal(0, error, len(self.survival_df))
                adjusted_max_conc = self.survival_df['æœ€å¤§æµ“åº¦'] + noise
                simulated_reaching += np.mean(adjusted_max_conc >= self.threshold)
            
            avg_reaching_rate = simulated_reaching / total_simulations
            rate_change = (avg_reaching_rate - baseline_reaching_rate) / baseline_reaching_rate
            
            print(f"  è¯¯å·®Â±{error:.3f}: è¾¾æ ‡ç‡å˜åŒ– {rate_change:.1%}")
        
        # å­•å‘¨æµ‹é‡è¯¯å·®
        print("\nå­•å‘¨æµ‹é‡è¯¯å·®æ•æ„Ÿæ€§åˆ†æ:")
        week_errors = [0.5, 1.0, 1.5, 2.0]
        
        for week_error in week_errors:
            # è®¡ç®—æ—¶é—´ç‚¹æ¨èçš„å˜åŒ–
            baseline_mean_time = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ']==1]['äº‹ä»¶æ—¶é—´'].mean()
            time_change = week_error / baseline_mean_time
            print(f"  è¯¯å·®Â±{week_error}å‘¨: æ¨èæ—¶ç‚¹ç›¸å¯¹è¯¯å·® {time_change:.1%}")
    
    def create_innovative_visualizations(self):
        """åˆ›å»ºåˆ›æ–°æ€§å¯è§†åŒ–å›¾è¡¨"""
        print("\n" + "="*60)
        print("ç”Ÿæˆåˆ›æ–°æ€§å¯è§†åŒ–å›¾è¡¨")
        print("="*60)
        
        fig = plt.figure(figsize=(20, 15))
        
        # åˆ›å»ºå¤æ‚çš„å­å›¾å¸ƒå±€
        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)
        
        # 1. ç”Ÿå­˜åˆ†æä¸»å›¾ï¼šä¸åŒBMIç»„çš„ç´¯ç§¯è¾¾æ ‡æ¦‚ç‡
        ax1 = fig.add_subplot(gs[0, :2])
        
        colors = plt.cm.Set1(np.linspace(0, 1, len(self.survival_df['ä¼˜åŒ–BMIç»„'].unique())))
        
        for i, group_id in enumerate(sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique())):
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(observed_data) >= 3:
                times = observed_data['äº‹ä»¶æ—¶é—´'].values
                events = np.ones(len(times))
                
                unique_times, cumulative_prob = self.kaplan_meier_estimator(times, events)
                
                if len(unique_times) > 0:
                    # æ‰©å±•åˆ°æ›´å¤§èŒƒå›´
                    extended_times = np.concatenate([[10], unique_times, [25]])
                    extended_probs = np.concatenate([[0], cumulative_prob, [cumulative_prob[-1]]])
                    
                    ax1.plot(extended_times, extended_probs, 'o-', 
                            color=colors[i], label=f'BMIç»„{group_id}', linewidth=2, markersize=4)
                    
                    # æ·»åŠ 80%å’Œ90%ç½®ä¿¡çº¿
                    ax1.axhline(y=0.8, color='red', linestyle='--', alpha=0.5)
                    ax1.axhline(y=0.9, color='orange', linestyle='--', alpha=0.5)
        
        ax1.set_xlabel('å­•å‘¨')
        ax1.set_ylabel('ç´¯ç§¯è¾¾æ ‡æ¦‚ç‡')
        ax1.set_title('ä¸åŒBMIç»„çš„YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡ç”Ÿå­˜æ›²çº¿')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.text(22, 0.8, '80%ç½®ä¿¡çº¿', color='red', alpha=0.7)
        ax1.text(22, 0.9, '90%ç½®ä¿¡çº¿', color='orange', alpha=0.7)
        
        # 2. é£é™©çƒ­åŠ›å›¾
        ax2 = fig.add_subplot(gs[0, 2:])
        
        # åˆ›å»ºBMI-æ—¶é—´çš„é£é™©çŸ©é˜µ
        bmi_range = np.linspace(self.survival_df['BMI'].min(), self.survival_df['BMI'].max(), 20)
        time_range = np.linspace(10, 25, 15)
        
        risk_matrix = np.zeros((len(time_range), len(bmi_range)))
        
        for i, t in enumerate(time_range):
            for j, bmi in enumerate(bmi_range):
                # ç®€åŒ–çš„é£é™©å‡½æ•°
                early_risk = 0.3 * np.exp(-(t-12)**2/8)  # æ—©æœŸæ£€æµ‹é£é™©
                delay_risk = 0.1 + 0.8 * (1 / (1 + np.exp(-(t-20))))  # å»¶è¿Ÿé£é™©
                bmi_factor = 1 + 0.1 * (bmi - 30) / 10  # BMIå½±å“å› å­
                
                total_risk = (early_risk + delay_risk) * bmi_factor
                risk_matrix[i, j] = total_risk
        
        im = ax2.imshow(risk_matrix, cmap='YlOrRd', aspect='auto', origin='lower')
        ax2.set_xticks(np.arange(0, len(bmi_range), 3))
        ax2.set_xticklabels([f'{bmi:.1f}' for bmi in bmi_range[::3]])
        ax2.set_yticks(np.arange(0, len(time_range), 2))
        ax2.set_yticklabels([f'{t:.0f}' for t in time_range[::2]])
        ax2.set_xlabel('BMI')
        ax2.set_ylabel('æ£€æµ‹æ—¶ç‚¹(å‘¨)')
        ax2.set_title('BMI-æ£€æµ‹æ—¶ç‚¹é£é™©çƒ­åŠ›å›¾')
        plt.colorbar(im, ax=ax2, label='ç»¼åˆé£é™©')
        
        # 3. 3Dæ•£ç‚¹å›¾ï¼šBMI-æ—¶é—´-æµ“åº¦å…³ç³»
        ax3 = fig.add_subplot(gs[1, :2], projection='3d')
        
        observed_data = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1]
        scatter = ax3.scatter(observed_data['BMI'], observed_data['äº‹ä»¶æ—¶é—´'], 
                             observed_data['æœ€å¤§æµ“åº¦'], 
                             c=observed_data['ä¼˜åŒ–BMIç»„'], cmap='viridis', 
                             alpha=0.6, s=30)
        
        ax3.set_xlabel('BMI')
        ax3.set_ylabel('è¾¾æ ‡æ—¶é—´(å‘¨)')
        ax3.set_zlabel('æœ€å¤§YæŸ“è‰²ä½“æµ“åº¦')
        ax3.set_title('BMI-è¾¾æ ‡æ—¶é—´-æµ“åº¦3Då…³ç³»')
        
        # 4. å†³ç­–è¾¹ç•Œå¯è§†åŒ–
        ax4 = fig.add_subplot(gs[1, 2:])
        
        # ç»˜åˆ¶BMIåˆ†ç»„çš„å†³ç­–è¾¹ç•Œ
        bmi_data = self.survival_df['BMI'].values
        group_data = self.survival_df['ä¼˜åŒ–BMIç»„'].values
        
        for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
            group_bmis = bmi_data[group_data == group_id]
            if len(group_bmis) > 0:
                ax4.hist(group_bmis, bins=15, alpha=0.6, 
                        label=f'BMIç»„{group_id}', density=True)
        
        ax4.set_xlabel('BMI')
        ax4.set_ylabel('å¯†åº¦')
        ax4.set_title('BMIåˆ†ç»„å†³ç­–è¾¹ç•Œ')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. æ¨èæ—¶ç‚¹å¯¹æ¯”é›·è¾¾å›¾
        ax5 = fig.add_subplot(gs[2, :2], projection='polar')
        
        if hasattr(self, 'recommendations_df') and len(self.recommendations_df) > 0:
            groups = self.recommendations_df['BMIç»„'].values
            timepoints = [float(x.split('å‘¨')[0]) for x in self.recommendations_df['æ¨èæ£€æµ‹æ—¶ç‚¹']]
            
            # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´ç”¨äºé›·è¾¾å›¾
            normalized_times = [(t-10)/(25-10) for t in timepoints]
            
            angles = np.linspace(0, 2*np.pi, len(groups), endpoint=False)
            normalized_times += normalized_times[:1]  # é—­åˆå›¾å½¢
            angles = np.concatenate([angles, [angles[0]]])
            
            ax5.plot(angles, normalized_times, 'o-', linewidth=2, color='blue')
            ax5.fill(angles, normalized_times, alpha=0.25, color='blue')
            ax5.set_xticks(angles[:-1])
            ax5.set_xticklabels([f'BMIç»„{g}' for g in groups])
            ax5.set_ylim(0, 1)
            ax5.set_title('å„BMIç»„æ¨èæ£€æµ‹æ—¶ç‚¹\n(é›·è¾¾å›¾)', pad=20)
        
        # 6. è¯¯å·®æ•æ„Ÿæ€§åˆ†æå›¾
        ax6 = fig.add_subplot(gs[2, 2:])
        
        # æ¨¡æ‹Ÿä¸åŒè¯¯å·®æ°´å¹³çš„å½±å“
        error_levels = np.linspace(0, 0.01, 11)
        impact_on_success_rate = []
        impact_on_timepoint = []
        
        baseline_rate = self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].mean()
        baseline_time = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ']==1]['äº‹ä»¶æ—¶é—´'].mean()
        
        for error in error_levels:
            # ç®€åŒ–çš„è¯¯å·®å½±å“æ¨¡å‹
            rate_impact = -error * 100  # æµ“åº¦è¯¯å·®å¯¹æˆåŠŸç‡çš„å½±å“
            time_impact = error * 50   # å¯¹æ—¶é—´æ¨èçš„å½±å“
            
            impact_on_success_rate.append(rate_impact)
            impact_on_timepoint.append(time_impact)
        
        ax6_twin = ax6.twinx()
        
        line1 = ax6.plot(error_levels*1000, impact_on_success_rate, 'b-o', 
                        label='è¾¾æ ‡ç‡å˜åŒ–(%)', linewidth=2)
        line2 = ax6_twin.plot(error_levels*1000, impact_on_timepoint, 'r-s', 
                             label='æ—¶ç‚¹æ¨èå˜åŒ–(%)', linewidth=2)
        
        ax6.set_xlabel('YæŸ“è‰²ä½“æµ“åº¦æµ‹é‡è¯¯å·® (â€°)')
        ax6.set_ylabel('è¾¾æ ‡ç‡å˜åŒ– (%)', color='blue')
        ax6_twin.set_ylabel('æ—¶ç‚¹æ¨èå˜åŒ– (%)', color='red')
        ax6.set_title('æ£€æµ‹è¯¯å·®æ•æ„Ÿæ€§åˆ†æ')
        ax6.grid(True, alpha=0.3)
        
        # åˆå¹¶å›¾ä¾‹
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax6.legend(lines, labels, loc='upper left')
        
        plt.tight_layout()
        plt.savefig('innovative_nipt_survival_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("åŸºäºç”Ÿå­˜åˆ†æçš„NIPTä¼˜åŒ–æ–¹æ¡ˆ - ç»¼åˆæŠ¥å‘Š")
        print("="*80)
        
        print("\nğŸ“Š æ•°æ®æ¦‚å†µä¸ç”Ÿå­˜åˆ†æç»“æœ:")
        print(f"â€¢ æ€»æ ·æœ¬æ•°: {len(self.survival_df)}åå­•å¦‡")
        print(f"â€¢ è§‚å¯Ÿåˆ°è¾¾æ ‡äº‹ä»¶: {self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].sum()}ä¾‹")
        print(f"â€¢ å³åˆ å¤±(æœªè¾¾æ ‡): {self.survival_df['å³åˆ å¤±'].sum()}ä¾‹")
        print(f"â€¢ åŒºé—´åˆ å¤±: {self.survival_df['åŒºé—´åˆ å¤±'].sum()}ä¾‹")
        print(f"â€¢ æ€»ä½“è¾¾æ ‡ç‡: {self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].mean():.1%}")
        
        observed_data = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1]
        print(f"â€¢ å¹³å‡è¾¾æ ‡æ—¶é—´: {observed_data['äº‹ä»¶æ—¶é—´'].mean():.1f}å‘¨")
        print(f"â€¢ è¾¾æ ‡æ—¶é—´ä¸­ä½æ•°: {observed_data['äº‹ä»¶æ—¶é—´'].median():.1f}å‘¨")
        print(f"â€¢ è¾¾æ ‡æ—¶é—´èŒƒå›´: {observed_data['äº‹ä»¶æ—¶é—´'].min():.1f}-{observed_data['äº‹ä»¶æ—¶é—´'].max():.1f}å‘¨")
        
        print("\nğŸ¯ åŸºäºç”Ÿå­˜åˆ†æçš„BMIåˆ†ç»„ä¸æ£€æµ‹æ—¶ç‚¹æ¨è:")
        print("-" * 80)
        
        if hasattr(self, 'recommendations_df'):
            for _, rec in self.recommendations_df.iterrows():
                print(f"\nğŸ“‹ BMIç»„ {rec['BMIç»„']} {rec['BMIåŒºé—´']}:")
                print(f"   â€¢ æ ·æœ¬æ„æˆ: æ€»æ•°{rec['æ€»æ ·æœ¬æ•°']}äºº, è¾¾æ ‡{rec['è¾¾æ ‡æ ·æœ¬æ•°']}äºº")
                print(f"   â€¢ è¾¾æ ‡ç‡: {rec['è¾¾æ ‡ç‡']}")
                print(f"   â€¢ å¹³å‡è¾¾æ ‡æ—¶é—´: {rec['å¹³å‡è¾¾æ ‡æ—¶é—´']}")
                print(f"   â€¢ ç”Ÿå­˜åˆ†ææ¨èæ—¶ç‚¹:")
                print(f"     - 80%åˆ†ä½æ•°æ—¶ç‚¹: {rec['80%åˆ†ä½æ•°æ—¶ç‚¹']}")
                print(f"     - 90%åˆ†ä½æ•°æ—¶ç‚¹: {rec['90%åˆ†ä½æ•°æ—¶ç‚¹']}")
                print(f"     - é£é™©æœ€ä¼˜æ—¶ç‚¹: {rec['é£é™©æœ€ä¼˜æ—¶ç‚¹']}")
                print(f"   â€¢ ğŸ¯ æœ€ç»ˆæ¨èæ£€æµ‹æ—¶ç‚¹: {rec['æ¨èæ£€æµ‹æ—¶ç‚¹']}")
                print(f"   â€¢ é¢„æœŸé£é™©æ°´å¹³: {rec['æœ€å°é£é™©å€¼']}")
        
        print("\nğŸ’¡ åˆ›æ–°æ€§åˆ†ææ–¹æ³•æ€»ç»“:")
        print("1. ç”Ÿå­˜åˆ†ææ–¹æ³•:")
        print("   - è€ƒè™‘äº†å³åˆ å¤±å’ŒåŒºé—´åˆ å¤±æ•°æ®")
        print("   - ä½¿ç”¨Kaplan-Meierä¼°è®¡å™¨è®¡ç®—ç´¯ç§¯è¾¾æ ‡æ¦‚ç‡")
        print("   - åŸºäºåˆ†ä½æ•°ç¡®å®šæœ€ä½³æ£€æµ‹æ—¶ç‚¹")
        
        print("\n2. å†³ç­–æ ‘ä¼˜åŒ–åˆ†ç»„:")
        print("   - æ•°æ®é©±åŠ¨çš„BMIåˆ†ç»„è¾¹ç•Œç¡®å®š")
        print("   - äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜åˆ†ç»„æ•°é‡")
        print("   - å¹³è¡¡å„ç»„æ ·æœ¬é‡ä¸é¢„æµ‹ç²¾åº¦")
        
        print("\n3. å¤šç›®æ ‡é£é™©ä¼˜åŒ–:")
        print("   - å¹³è¡¡æ—©æœŸæ£€æµ‹å¤±è´¥é£é™©ä¸å»¶è¿Ÿå‘ç°é£é™©")
        print("   - è€ƒè™‘æ²»ç–—çª—å£æœŸçš„ä¸´åºŠçº¦æŸ")
        print("   - åŸºäºé£é™©å‡½æ•°çš„æ•°å­¦ä¼˜åŒ–")
        
        print("\n4. æ•æ„Ÿæ€§åˆ†æ:")
        print("   - æµ‹é‡è¯¯å·®å¯¹ç»“æœç¨³å¥æ€§çš„å½±å“è¯„ä¼°")
        print("   - Monte Carloæ¨¡æ‹ŸéªŒè¯æ¨èæ–¹æ¡ˆ")
        
        print("\nğŸ¥ ä¸´åºŠåº”ç”¨å»ºè®®:")
        print("1. ä¸ªæ€§åŒ–æ£€æµ‹ç­–ç•¥: æ ¹æ®å­•å¦‡BMIé€‰æ‹©æœ€ä¼˜æ£€æµ‹æ—¶ç‚¹")
        print("2. è´¨é‡æ§åˆ¶: é‡ç‚¹æ§åˆ¶YæŸ“è‰²ä½“æµ“åº¦æµ‹é‡ç²¾åº¦")
        print("3. åŠ¨æ€ç›‘æµ‹: å¯¹é«˜é£é™©ç»„å»ºè®®å¤šæ¬¡æ£€æµ‹")
        print("4. æ—¶é—´çª—å£: ç¡®ä¿æ‰€æœ‰æ£€æµ‹åœ¨20å‘¨å‰å®Œæˆ")
        
        print("\nğŸ“ˆ æ–¹æ³•å­¦ä¼˜åŠ¿:")
        print("â€¢ å¤„ç†åˆ å¤±æ•°æ®ï¼Œå……åˆ†åˆ©ç”¨æ‰€æœ‰å¯ç”¨ä¿¡æ¯")
        print("â€¢ æ•°æ®é©±åŠ¨çš„åˆ†ç»„ç­–ç•¥ï¼Œé¿å…ä¸»è§‚åˆ’åˆ†")
        print("â€¢ å¤šç›®æ ‡ä¼˜åŒ–ï¼Œå¹³è¡¡ä¸åŒç±»å‹é£é™©")
        print("â€¢ ç¨³å¥æ€§éªŒè¯ï¼Œç¡®ä¿æ¨èæ–¹æ¡ˆå¯é ")
        
        # ä¿å­˜ç»“æœ
        if hasattr(self, 'recommendations_df'):
            self.recommendations_df.to_excel('survival_based_recommendations.xlsx', index=False)
            print("\nâœ… è¯¦ç»†æ¨èæ–¹æ¡ˆå·²ä¿å­˜è‡³: survival_based_recommendations.xlsx")
        
        return True
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„ç”Ÿå­˜åˆ†æ"""
        print("NIPTé—®é¢˜2: åŸºäºç”Ÿå­˜åˆ†æçš„BMIåˆ†ç»„ä¸æ£€æµ‹æ—¶ç‚¹ä¼˜åŒ–")
        print("="*80)
        
        # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        if not self.load_and_process_data():
            return False
        
        # 2. BMIåˆ†ç»„ä¼˜åŒ–
        self.optimize_bmi_grouping()
        
        # 3. è®¡ç®—æœ€ä½³æ£€æµ‹æ—¶ç‚¹
        self.calculate_optimal_timepoints()
        
        # 4. æ•æ„Ÿæ€§åˆ†æ
        self.sensitivity_analysis()
        
        # 5. åˆ›æ–°æ€§å¯è§†åŒ–
        self.create_innovative_visualizations()
        
        # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report()
        
        return True

# ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    analyzer = SurvivalBasedNIPTOptimizer()
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\n" + "="*80)
        print("ğŸ‰ åŸºäºç”Ÿå­˜åˆ†æçš„NIPTä¼˜åŒ–åˆ†æå®Œæˆï¼")
        print("="*80)
        print("æ ¸å¿ƒåˆ›æ–°:")
        print("âœ… å¼•å…¥ç”Ÿå­˜åˆ†æå¤„ç†åˆ å¤±æ•°æ®")
        print("âœ… å†³ç­–æ ‘ä¼˜åŒ–BMIåˆ†ç»„ç­–ç•¥") 
        print("âœ… å¤šç›®æ ‡é£é™©å‡½æ•°ä¼˜åŒ–æ£€æµ‹æ—¶ç‚¹")
        print("âœ… åˆ›æ–°æ€§å¯è§†åŒ–å±•ç¤ºåˆ†æç»“æœ")
        print("âœ… å…¨é¢çš„æ•æ„Ÿæ€§åˆ†æéªŒè¯")
    else:
        print("âŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
