# NIPTé—®é¢˜2ï¼šä¼˜åŒ–åˆ†ç»„ + ç”Ÿå­˜åˆ†æä¼˜åŒ–æ–¹æ¡ˆ
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.optimize import minimize_scalar
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
import re
import traceback
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (15, 10)

class ImprovedNIPTOptimizer:
    """æ”¹è¿›çš„NIPTä¼˜åŒ–å™¨ï¼šä¼˜åŒ–åˆ†ç»„ + ç”Ÿå­˜åˆ†æ"""
    
    def __init__(self, data_file='é™„ä»¶.xlsx'):
        self.data_file = data_file
        self.threshold = 0.04
        self.alpha = 0.1  # 90%ç½®ä¿¡æ°´å¹³
        
    def load_and_process_data(self):
        """åŠ è½½å¹¶å¤„ç†æ•°æ®"""
        print("="*80)
        print("æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
        print("="*80)
        
        try:
            # è¯»å–åŸå§‹æ•°æ®
            original_data = pd.read_excel(self.data_file, sheet_name=0)
            print(f"æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŸå§‹æ•°æ®å½¢çŠ¶: {original_data.shape}")
            
            # è½¬æ¢å­•å‘¨æ ¼å¼
            def convert_gestation_week(week_str):
                if pd.isna(week_str):
                    return np.nan
                week_str = str(week_str).strip()
                pattern = r'(\d+)(?:w\+?(\d+))?|(\d+\.\d+)'
                match = re.search(pattern, week_str.lower())
                
                if match:
                    if match.group(3):  # å·²ç»æ˜¯å°æ•°æ ¼å¼
                        return float(match.group(3))
                    else:
                        weeks = int(match.group(1))
                        days = int(match.group(2)) if match.group(2) else 0
                        return weeks + days/7
                
                try:
                    return float(week_str)
                except ValueError:
                    return np.nan
            
            original_data['å­•å‘¨_æ•°å€¼'] = original_data['æ£€æµ‹å­•å‘¨'].apply(convert_gestation_week)
            
            # è¿‡æ»¤ç”·èƒæ•°æ®
            male_data = original_data[original_data['YæŸ“è‰²ä½“æµ“åº¦'] > 0].copy()
            male_data = male_data.dropna(subset=['å­•å‘¨_æ•°å€¼', 'å­•å¦‡BMI', 'YæŸ“è‰²ä½“æµ“åº¦'])
            
            print(f"ç”·èƒæ•°æ®ç­›é€‰å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬: {len(male_data)}")
            
            # æ„å»ºç”Ÿå­˜æ•°æ®
            survival_data = []
            
            for woman_code in male_data['å­•å¦‡ä»£ç '].unique():
                woman_data = male_data[male_data['å­•å¦‡ä»£ç '] == woman_code].copy()
                woman_data = woman_data.sort_values('å­•å‘¨_æ•°å€¼')
                
                if len(woman_data) == 0:
                    continue
                
                bmi = woman_data['å­•å¦‡BMI'].iloc[0]
                age = woman_data['å¹´é¾„'].iloc[0] if 'å¹´é¾„' in woman_data.columns else np.nan
                
                # ç¡®å®šäº‹ä»¶æ—¶é—´å’Œåˆ å¤±çŠ¶æ€
                reaching_records = woman_data[woman_data['YæŸ“è‰²ä½“æµ“åº¦'] >= self.threshold]
                
                if len(reaching_records) > 0:
                    # è§‚å¯Ÿåˆ°äº‹ä»¶ï¼ˆè¾¾æ ‡ï¼‰
                    event_time = reaching_records['å­•å‘¨_æ•°å€¼'].iloc[0]
                    censored = 0  # æœªåˆ å¤±
                    event_observed = 1
                else:
                    # å³åˆ å¤±ï¼šæœªè§‚å¯Ÿåˆ°è¾¾æ ‡äº‹ä»¶
                    event_time = woman_data['å­•å‘¨_æ•°å€¼'].max()
                    censored = 1  # å³åˆ å¤±
                    event_observed = 0
                
                # æ£€æŸ¥åŒºé—´åˆ å¤±
                interval_censored = 0
                lower_bound = event_time
                upper_bound = event_time
                
                if event_observed == 1 and len(woman_data) > 1:
                    # æ£€æŸ¥æ˜¯å¦åœ¨ä¸¤æ¬¡æ£€æµ‹ä¹‹é—´è¾¾æ ‡
                    prev_records = woman_data[woman_data['å­•å‘¨_æ•°å€¼'] < event_time]
                    if len(prev_records) > 0:
                        last_below = prev_records.iloc[-1]
                        if last_below['YæŸ“è‰²ä½“æµ“åº¦'] < self.threshold:
                            # åŒºé—´åˆ å¤±ï¼šåœ¨(last_time, event_time]ä¹‹é—´è¾¾æ ‡
                            interval_censored = 1
                            lower_bound = last_below['å­•å‘¨_æ•°å€¼']
                            upper_bound = event_time
                
                survival_data.append({
                    'å­•å¦‡ä»£ç ': woman_code,
                    'BMI': bmi,
                    'å¹´é¾„': age,
                    'äº‹ä»¶æ—¶é—´': event_time,
                    'äº‹ä»¶è§‚å¯Ÿ': event_observed,
                    'å³åˆ å¤±': censored,
                    'åŒºé—´åˆ å¤±': interval_censored,
                    'ä¸‹ç•Œ': lower_bound,
                    'ä¸Šç•Œ': upper_bound,
                    'æœ€å¤§æµ“åº¦': woman_data['YæŸ“è‰²ä½“æµ“åº¦'].max(),
                    'æ£€æµ‹æ¬¡æ•°': len(woman_data)
                })
            
            self.survival_df = pd.DataFrame(survival_data)
            self.original_male_data = male_data
            
            print(f"ç”Ÿå­˜æ•°æ®æ„å»ºå®Œæˆ:")
            print(f"æ€»å­•å¦‡æ•°: {len(self.survival_df)}")
            print(f"è§‚å¯Ÿåˆ°è¾¾æ ‡äº‹ä»¶: {self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].sum()}")
            print(f"å³åˆ å¤±: {self.survival_df['å³åˆ å¤±'].sum()}")
            print(f"åŒºé—´åˆ å¤±: {self.survival_df['åŒºé—´åˆ å¤±'].sum()}")
            print(f"è¾¾æ ‡ç‡: {self.survival_df['äº‹ä»¶è§‚å¯Ÿ'].mean():.1%}")
            
            return True
            
        except Exception as e:
            print(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def analyze_bmi_concentration_relationship(self):
        """åˆ†æBMIä¸YæŸ“è‰²ä½“æµ“åº¦çš„å…³ç³»"""
        print("\n" + "="*60)
        print("æ­¥éª¤1: åˆ†æBMIä¸YæŸ“è‰²ä½“æµ“åº¦çš„çœŸå®å…³ç³»")
        print("="*60)
        
        # ä½¿ç”¨åŸå§‹æ•°æ®åˆ†æBMIä¸æµ“åº¦çš„å…³ç³»
        analysis_data = self.original_male_data.copy()
        
        print(f"åˆ†ææ ·æœ¬æ•°: {len(analysis_data)}")
        print(f"BMIèŒƒå›´: [{analysis_data['å­•å¦‡BMI'].min():.1f}, {analysis_data['å­•å¦‡BMI'].max():.1f}]")
        print(f"YæŸ“è‰²ä½“æµ“åº¦èŒƒå›´: [{analysis_data['YæŸ“è‰²ä½“æµ“åº¦'].min():.4f}, {analysis_data['YæŸ“è‰²ä½“æµ“åº¦'].max():.4f}]")
        
        # 1. åˆ†ææ¯ä¸ªBMIæ•´æ•°å€¼çš„å¹³å‡æµ“åº¦
        bmi_concentration_stats = []
        for bmi_val in range(int(analysis_data['å­•å¦‡BMI'].min()), int(analysis_data['å­•å¦‡BMI'].max()) + 1):
            bmi_data = analysis_data[(analysis_data['å­•å¦‡BMI'] >= bmi_val) & 
                                   (analysis_data['å­•å¦‡BMI'] < bmi_val + 1)]
            if len(bmi_data) >= 5:  # è‡³å°‘5ä¸ªæ ·æœ¬
                stats_dict = {
                    'BMIåŒºé—´': f'[{bmi_val}, {bmi_val+1})',
                    'BMIä¸­å¿ƒ': bmi_val + 0.5,
                    'æ ·æœ¬æ•°': len(bmi_data),
                    'å¹³å‡æµ“åº¦': bmi_data['YæŸ“è‰²ä½“æµ“åº¦'].mean(),
                    'æµ“åº¦ä¸­ä½æ•°': bmi_data['YæŸ“è‰²ä½“æµ“åº¦'].median(),
                    'æµ“åº¦æ ‡å‡†å·®': bmi_data['YæŸ“è‰²ä½“æµ“åº¦'].std(),
                    'è¾¾æ ‡ç‡': (bmi_data['YæŸ“è‰²ä½“æµ“åº¦'] >= self.threshold).mean()
                }
                bmi_concentration_stats.append(stats_dict)
                print(f"BMI {bmi_val}-{bmi_val+1}: æ ·æœ¬æ•°={len(bmi_data)}, "
                      f"å¹³å‡æµ“åº¦={stats_dict['å¹³å‡æµ“åº¦']:.4f}, è¾¾æ ‡ç‡={stats_dict['è¾¾æ ‡ç‡']:.1%}")
        
        self.bmi_concentration_stats = pd.DataFrame(bmi_concentration_stats)
        
        # 2. å¯»æ‰¾æµ“åº¦å³°å€¼
        if len(self.bmi_concentration_stats) > 0:
            max_concentration_idx = self.bmi_concentration_stats['å¹³å‡æµ“åº¦'].idxmax()
            peak_bmi = self.bmi_concentration_stats.loc[max_concentration_idx, 'BMIä¸­å¿ƒ']
            peak_concentration = self.bmi_concentration_stats.loc[max_concentration_idx, 'å¹³å‡æµ“åº¦']
            
            print(f"\nå…³é”®å‘ç°:")
            print(f"YæŸ“è‰²ä½“æµ“åº¦å³°å€¼BMI: {peak_bmi:.1f}")
            print(f"å³°å€¼æµ“åº¦: {peak_concentration:.4f}")
            
            self.peak_bmi = peak_bmi
            self.peak_concentration = peak_concentration
            
            return True
        else:
            print("âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æBMI-æµ“åº¦å…³ç³»")
            return False
    
    def optimized_bmi_grouping(self):
        """ä¼˜åŒ–çš„BMIåˆ†ç»„æ–¹æ³•"""
        print("\n" + "="*60)
        print("æ­¥éª¤2: ä¼˜åŒ–çš„BMIåˆ†ç»„")
        print("="*60)
        
        observed_data = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1].copy()
        
        if len(observed_data) < 20:
            print("âŒ æ ·æœ¬æ•°ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç»„")
            return self.fallback_grouping()
        
        # æ–¹æ³•1: åŸºäºå†³ç­–æ ‘çš„åˆ†ç»„
        print("æ–¹æ³•1: åŸºäºå†³ç­–æ ‘çš„æœ€ä¼˜åˆ†ç»„")
        
        # å‡†å¤‡æ•°æ®
        X = observed_data[['BMI']].values
        y = observed_data['äº‹ä»¶æ—¶é—´'].values
        
        # ç½‘æ ¼æœç´¢æœ€ä¼˜å†³ç­–æ ‘å‚æ•°
        param_grid = {
            'max_depth': [2, 3, 4, 5],
            'min_samples_split': [10, 15, 20],
            'min_samples_leaf': [5, 8, 10]
        }
        
        tree = DecisionTreeRegressor(random_state=42)
        grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='neg_mean_squared_error')
        grid_search.fit(X, y)
        
        best_tree = grid_search.best_estimator_
        print(f"æœ€ä¼˜å†³ç­–æ ‘å‚æ•°: {grid_search.best_params_}")
        print(f"äº¤å‰éªŒè¯å¾—åˆ†: {-grid_search.best_score_:.3f}")
        
        # è·å–åˆ†å‰²ç‚¹
        tree_splits = self.extract_tree_splits(best_tree, X, y)
        print(f"å†³ç­–æ ‘åˆ†å‰²ç‚¹: {tree_splits}")
        
        # æ–¹æ³•2: åŸºäºK-meansèšç±»çš„åˆ†ç»„
        print("\næ–¹æ³•2: åŸºäºK-meansèšç±»çš„åˆ†ç»„")
        best_k = self.find_optimal_clusters(observed_data)
        
        if best_k > 1:
            kmeans = KMeans(n_clusters=best_k, random_state=42)
            clusters = kmeans.fit_predict(observed_data[['BMI', 'äº‹ä»¶æ—¶é—´']])
            cluster_centers = kmeans.cluster_centers_
            
            # æŒ‰BMIæ’åºèšç±»ä¸­å¿ƒ
            sorted_centers = sorted(cluster_centers, key=lambda x: x[0])
            cluster_splits = []
            for i in range(len(sorted_centers)-1):
                split_point = (sorted_centers[i][0] + sorted_centers[i+1][0]) / 2
                cluster_splits.append(split_point)
            
            print(f"K-meansåˆ†å‰²ç‚¹ (k={best_k}): {cluster_splits}")
        else:
            cluster_splits = []
        
        # æ–¹æ³•3: åŸºäºåˆ†ä½æ•°çš„åˆ†ç»„
        print("\næ–¹æ³•3: åŸºäºåˆ†ä½æ•°çš„åˆ†ç»„")
        bmi_quartiles = observed_data['BMI'].quantile([0.25, 0.5, 0.75]).values
        quartile_splits = [25] + list(bmi_quartiles) + [50]  # æ·»åŠ è¾¹ç•Œå€¼
        quartile_splits = sorted(list(set(quartile_splits)))  # å»é‡å¹¶æ’åº
        print(f"åˆ†ä½æ•°åˆ†å‰²ç‚¹: {quartile_splits[1:-1]}")  # å»æ‰è¾¹ç•Œå€¼
        
        # ç»¼åˆè¯„ä¼°ä¸‰ç§æ–¹æ³•
        print("\næ–¹æ³•è¯„ä¼°ä¸é€‰æ‹©:")
        all_methods = [
            ('å†³ç­–æ ‘', tree_splits),
            ('K-means', cluster_splits),
            ('åˆ†ä½æ•°', quartile_splits[1:-1])  # å»æ‰è¾¹ç•Œå€¼
        ]
        
        best_method = None
        best_score = float('inf')
        
        for method_name, splits in all_methods:
            if len(splits) >= 2:  # è‡³å°‘è¦æœ‰2ä¸ªåˆ†å‰²ç‚¹ï¼ˆ3ç»„ï¼‰
                score = self.evaluate_grouping_quality(observed_data, splits)
                print(f"{method_name}åˆ†ç»„è´¨é‡è¯„åˆ†: {score:.3f}")
                
                if score < best_score:
                    best_score = score
                    best_method = (method_name, splits)
        
        if best_method:
            method_name, final_splits = best_method
            print(f"\nâœ… é€‰æ‹©æœ€ä¼˜æ–¹æ³•: {method_name}")
            print(f"æœ€ä¼˜åˆ†å‰²ç‚¹: {final_splits}")
            
            # åˆ›å»ºåˆ†ç»„
            self.create_data_driven_groups(final_splits)
            return True
        else:
            print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½ä¸é€‚ç”¨ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç»„")
            return self.fallback_grouping()
    
    def extract_tree_splits(self, tree, X, y):
        """ä»å†³ç­–æ ‘ä¸­æå–åˆ†å‰²ç‚¹"""
        tree_ = tree.tree_
        splits = []
        
        def recurse(node):
            if tree_.feature[node] != -2:  # ä¸æ˜¯å¶å­èŠ‚ç‚¹
                threshold = tree_.threshold[node]
                splits.append(threshold)
                recurse(tree_.children_left[node])
                recurse(tree_.children_right[node])
        
        recurse(0)
        return sorted(list(set(splits)))
    
    def find_optimal_clusters(self, data):
        """å¯»æ‰¾æœ€ä¼˜èšç±»æ•°"""
        X = data[['BMI', 'äº‹ä»¶æ—¶é—´']].values
        
        silhouette_scores = []
        k_range = range(2, min(8, len(data)//3))  # æœ€å¤š7ç»„ï¼Œæ¯ç»„è‡³å°‘3ä¸ªæ ·æœ¬
        
        for k in k_range:
            if k <= len(data):
                kmeans = KMeans(n_clusters=k, random_state=42)
                labels = kmeans.fit_predict(X)
                score = silhouette_score(X, labels)
                silhouette_scores.append(score)
                print(f"K={k}, è½®å»“ç³»æ•°={score:.3f}")
        
        if silhouette_scores:
            best_k = k_range[np.argmax(silhouette_scores)]
            print(f"æœ€ä¼˜èšç±»æ•°: K={best_k}")
            return best_k
        else:
            return 0
    
    def evaluate_grouping_quality(self, data, splits):
        """è¯„ä¼°åˆ†ç»„è´¨é‡"""
        try:
            # åˆ›å»ºåˆ†ç»„
            groups = self.assign_groups_by_splits(data['BMI'], splits)
            
            if len(set(groups)) < 2:
                return float('inf')
            
            # è®¡ç®—ç»„å†…æ–¹å·®å’Œç»„é—´æ–¹å·®æ¯”
            group_variances = []
            group_means = []
            
            for group_id in set(groups):
                group_data = data[groups == group_id]['äº‹ä»¶æ—¶é—´']
                if len(group_data) > 1:
                    group_variances.append(group_data.var())
                    group_means.append(group_data.mean())
            
            if len(group_variances) < 2:
                return float('inf')
            
            within_group_var = np.mean(group_variances)
            between_group_var = np.var(group_means)
            
            # è¯„åˆ†ï¼šç»„å†…æ–¹å·®è¶Šå°ï¼Œç»„é—´æ–¹å·®è¶Šå¤§ï¼Œè¯„åˆ†è¶Šä½ï¼ˆè¶Šå¥½ï¼‰
            score = within_group_var / (between_group_var + 1e-6)
            
            return score
            
        except:
            return float('inf')
    
    def assign_groups_by_splits(self, bmi_values, splits):
        """æ ¹æ®åˆ†å‰²ç‚¹åˆ†é…ç»„åˆ«"""
        groups = np.zeros(len(bmi_values))
        
        for i, bmi in enumerate(bmi_values):
            group_id = 1
            for split in sorted(splits):
                if bmi >= split:
                    group_id += 1
                else:
                    break
            groups[i] = group_id
        
        return groups
    
    def create_data_driven_groups(self, splits):
        """æ ¹æ®ä¼˜åŒ–çš„åˆ†å‰²ç‚¹åˆ›å»ºåˆ†ç»„"""
        # æ·»åŠ è¾¹ç•Œå€¼
        full_splits = [0] + sorted(splits) + [100]
        
        def assign_group(bmi):
            if pd.isna(bmi):
                return 0
            
            for i, split in enumerate(full_splits[:-1]):
                if bmi >= split and bmi < full_splits[i+1]:
                    return i + 1
            return len(full_splits) - 1
        
        self.survival_df['ä¼˜åŒ–BMIç»„'] = self.survival_df['BMI'].apply(assign_group)
        
        # æ˜¾ç¤ºåˆ†ç»„ç»“æœ
        print(f"\nä¼˜åŒ–çš„BMIåˆ†ç»„ç»“æœ:")
        for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
            if group_id == 0:
                continue
            
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(group_data) > 0:
                bmi_min = group_data['BMI'].min()
                bmi_max = group_data['BMI'].max()
                avg_timing = observed_data['äº‹ä»¶æ—¶é—´'].mean() if len(observed_data) > 0 else np.nan
                reach_rate = observed_data.shape[0] / group_data.shape[0]
                
                print(f"ç»„{group_id}: BMI[{bmi_min:.1f}, {bmi_max:.1f}], "
                      f"æ ·æœ¬æ•°={len(group_data)}, è¾¾æ ‡ç‡={reach_rate:.1%}, "
                      f"å¹³å‡è¾¾æ ‡æ—¶é—´={avg_timing:.1f}å‘¨")
        
        # è®¡ç®—åŸºäºæ•°æ®çš„é£é™©å› å­
        self.calculate_optimized_risk_factors()
        
        return True
    
    def calculate_optimized_risk_factors(self):
        """åŸºäºçœŸå®æ•°æ®è®¡ç®—é£é™©å› å­"""
        print(f"\nè®¡ç®—ä¼˜åŒ–çš„é£é™©å› å­:")
        
        # è®¡ç®—å„ç»„çš„å®é™…é£é™©æŒ‡æ ‡
        group_risk_factors = {}
        
        for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
            if group_id == 0:
                continue
                
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(group_data) > 0:
                # é£é™©å› å­åŸºäºï¼š1-è¾¾æ ‡ç‡ + æ ‡å‡†åŒ–è¾¾æ ‡æ—¶é—´
                reach_rate = len(observed_data) / len(group_data)
                avg_timing = observed_data['äº‹ä»¶æ—¶é—´'].mean() if len(observed_data) > 0 else 20.0
                
                # æ ‡å‡†åŒ–è¾¾æ ‡æ—¶é—´ï¼ˆç›¸å¯¹äºå…¨ä½“å¹³å‡ï¼‰
                overall_avg_timing = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1]['äº‹ä»¶æ—¶é—´'].mean()
                timing_risk = (avg_timing - overall_avg_timing) / overall_avg_timing
                
                # ç»¼åˆé£é™©å› å­
                risk_factor = (1 - reach_rate) * 2 + max(0, timing_risk) * 1.5 + 0.5
                risk_factor = max(0.3, min(2.0, risk_factor))  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
                
                group_risk_factors[group_id] = risk_factor
                print(f"ç»„{group_id}: è¾¾æ ‡ç‡={reach_rate:.1%}, å¹³å‡è¾¾æ ‡æ—¶é—´={avg_timing:.1f}å‘¨, é£é™©å› å­={risk_factor:.2f}")
        
        self.optimized_risk_factors = group_risk_factors
        
        # æ›´æ–°survival_dfä¸­çš„é£é™©å› å­
        def get_optimized_risk_factor(bmi):
            if pd.isna(bmi):
                return 1.0
            
            # æ‰¾åˆ°å¯¹åº”çš„ç»„
            for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
                group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
                if len(group_data) > 0:
                    bmi_min = group_data['BMI'].min()
                    bmi_max = group_data['BMI'].max()
                    if bmi_min <= bmi <= bmi_max:
                        return self.optimized_risk_factors.get(group_id, 1.0)
            
            return 1.0  # é»˜è®¤å€¼
        
        self.survival_df['ä¼˜åŒ–é£é™©å› å­'] = self.survival_df['BMI'].apply(get_optimized_risk_factor)
    
    def fallback_grouping(self):
        """å¤‡ç”¨åˆ†ç»„æ–¹æ³•ï¼ˆå½“æ•°æ®ä¸è¶³æ—¶ï¼‰"""
        print("ä½¿ç”¨å¤‡ç”¨åˆ†ç»„æ–¹æ³•ï¼ˆåŸºäºåŒ»å­¦æ ‡å‡†ï¼‰")
        
        def assign_fallback_group(bmi):
            if pd.isna(bmi):
                return 0
            elif bmi < 28:
                return 1
            elif bmi < 32:
                return 2
            elif bmi < 36:
                return 3
            else:
                return 4
        
        self.survival_df['ä¼˜åŒ–BMIç»„'] = self.survival_df['BMI'].apply(assign_fallback_group)
        
        # ç®€å•çš„é£é™©å› å­
        self.optimized_risk_factors = {1: 1.2, 2: 0.9, 3: 1.1, 4: 1.4}
        self.survival_df['ä¼˜åŒ–é£é™©å› å­'] = self.survival_df['BMI'].apply(
            lambda bmi: self.optimized_risk_factors.get(assign_fallback_group(bmi), 1.0)
        )
        
        return True
    
    def kaplan_meier_estimator(self, times, events):
        """ç®€åŒ–çš„Kaplan-Meierä¼°è®¡å™¨"""
        # æ’åº
        sorted_indices = np.argsort(times)
        sorted_times = times[sorted_indices]
        sorted_events = events[sorted_indices]
        
        # è®¡ç®—ç”Ÿå­˜å‡½æ•°
        unique_times = np.unique(sorted_times[sorted_events == 1])
        survival_prob = []
        
        n = len(times)
        for t in unique_times:
            # åœ¨æ—¶é—´tå‘ç”Ÿäº‹ä»¶çš„æ•°é‡
            events_at_t = np.sum((sorted_times == t) & (sorted_events == 1))
            # åœ¨æ—¶é—´tä¹‹å‰çš„é£é™©é›†å¤§å°
            at_risk = np.sum(sorted_times >= t)
            
            if at_risk > 0:
                survival_prob.append(1 - events_at_t / at_risk)
            else:
                survival_prob.append(1.0)
        
        # ç´¯ç§¯ç”Ÿå­˜æ¦‚ç‡
        cumulative_survival = np.cumprod(survival_prob)
        
        return unique_times, 1 - cumulative_survival  # è¿”å›ç´¯ç§¯å‘ç”Ÿæ¦‚ç‡
    
    def calculate_optimal_timepoints(self):
        """è®¡ç®—å„ç»„æœ€ä½³æ£€æµ‹æ—¶ç‚¹ï¼ˆç»“åˆä¼˜åŒ–åˆ†ç»„å’Œç”Ÿå­˜åˆ†æï¼‰"""
        print("\n" + "="*60)
        print("æ­¥éª¤3: è®¡ç®—ä¼˜åŒ–çš„æœ€ä½³æ£€æµ‹æ—¶ç‚¹")
        print("="*60)
        
        recommendations = []
        
        for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
            if group_id == 0:
                continue
                
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(observed_data) < 2:
                print(f"ç»„{group_id}æ ·æœ¬é‡è¿‡å°ï¼Œè·³è¿‡")
                continue
            
            # è·å–ç»„ä¿¡æ¯
            bmi_min = group_data['BMI'].min()
            bmi_max = group_data['BMI'].max()
            group_name = f'BMI[{bmi_min:.1f},{bmi_max:.1f})ç»„'
            
            # Kaplan-Meierä¼°è®¡
            times = observed_data['äº‹ä»¶æ—¶é—´'].values
            events = np.ones(len(times))
            
            unique_times, cumulative_prob = self.kaplan_meier_estimator(times, events)
            
            # è®¡ç®—åˆ†ä½æ•°æ—¶ç‚¹
            percentile_80 = np.percentile(times, 80)
            percentile_90 = np.percentile(times, 90)
            
            # é£é™©ä¼˜åŒ–æ—¶ç‚¹
            risk_factor = self.optimized_risk_factors.get(group_id, 1.0)
            
            def risk_function(t):
                # æ—©æ£€æµ‹é£é™©ï¼šåœ¨æ—¶é—´tæ—¶æœªè¾¾æ ‡çš„æ¦‚ç‡
                early_risk = max(0.1, np.mean(times > t))
                
                # å»¶è¿Ÿå‘ç°é£é™©ï¼ˆè€ƒè™‘æ²»ç–—çª—å£æœŸï¼‰
                if t <= 12:
                    delay_risk = 0.05  # æ—©æœŸå‘ç°ï¼Œé£é™©å¾ˆä½
                elif t <= 16:
                    delay_risk = 0.15  # è¾ƒæ—©å‘ç°ï¼Œé£é™©è¾ƒä½
                elif t <= 20:
                    delay_risk = 0.35  # ä¸­æœŸå‘ç°ï¼Œé£é™©ä¸­ç­‰
                elif t <= 24:
                    delay_risk = 0.65  # è¾ƒæ™šå‘ç°ï¼Œé£é™©è¾ƒé«˜
                else:
                    delay_risk = 0.90  # å¾ˆæ™šå‘ç°ï¼Œé£é™©å¾ˆé«˜
                
                # ç»¼åˆé£é™©
                total_risk = 0.3 * early_risk * risk_factor + 0.7 * delay_risk
                return total_risk
            
            result = minimize_scalar(risk_function, bounds=(11, 22), method='bounded')
            optimal_time = result.x
            
            # æœ€ç»ˆæ¨è
            final_recommendation = min(
                (percentile_80 + optimal_time) / 2,
                20.0
            )
            final_recommendation = max(12.0, final_recommendation)
            
            recommendation = {
                'BMIç»„': group_id,
                'ç»„å': group_name,
                'BMIåŒºé—´': f"[{bmi_min:.1f}, {bmi_max:.1f}]",
                'æ€»æ ·æœ¬æ•°': len(group_data),
                'è¾¾æ ‡æ ·æœ¬æ•°': len(observed_data),
                'è¾¾æ ‡ç‡': f"{len(observed_data)/len(group_data):.1%}",
                'å¹³å‡è¾¾æ ‡æ—¶é—´': f"{observed_data['äº‹ä»¶æ—¶é—´'].mean():.1f}å‘¨",
                '80%åˆ†ä½æ•°æ—¶ç‚¹': f"{percentile_80:.1f}å‘¨",
                '90%åˆ†ä½æ•°æ—¶ç‚¹': f"{percentile_90:.1f}å‘¨",
                'é£é™©æœ€ä¼˜æ—¶ç‚¹': f"{optimal_time:.1f}å‘¨",
                'ä¼˜åŒ–é£é™©å› å­': f"{risk_factor:.2f}",
                'æ¨èæ£€æµ‹æ—¶ç‚¹': f"{final_recommendation:.1f}å‘¨",
                'æ–¹æ³•': 'ä¼˜åŒ–+ç”Ÿå­˜åˆ†æ'
            }
            
            recommendations.append(recommendation)
            
            print(f"\n{group_name}:")
            print(f"  æ ·æœ¬ç‰¹å¾: æ€»æ•°{len(group_data)}, è¾¾æ ‡{len(observed_data)}")
            print(f"  ä¼˜åŒ–é£é™©å› å­: {risk_factor:.2f}")
            print(f"  ğŸ¯ æ¨èæ£€æµ‹æ—¶ç‚¹: {final_recommendation:.1f}å‘¨")
        
        self.recommendations_df = pd.DataFrame(recommendations)
        return self.recommendations_df
    
    def create_bmi_concentration_plot(self):
        """åˆ›å»ºBMIä¸YæŸ“è‰²ä½“æµ“åº¦å…³ç³»å›¾ - æŒ‰ç…§task2é£æ ¼"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦å›¾ï¼šBMIä¸æµ“åº¦æ•£ç‚¹å›¾åŠæ‹Ÿåˆæ›²çº¿
        if hasattr(self, 'bmi_concentration_stats') and len(self.bmi_concentration_stats) > 0:
            # ä¸»æ•£ç‚¹å›¾
            scatter = ax1.scatter(self.bmi_concentration_stats['BMIä¸­å¿ƒ'], 
                                 self.bmi_concentration_stats['å¹³å‡æµ“åº¦'],
                                 s=self.bmi_concentration_stats['æ ·æœ¬æ•°']*2,
                                 alpha=0.7, c='steelblue', edgecolors='navy', linewidth=1.5)
            
            # æ‹Ÿåˆæ›²çº¿
            from scipy.optimize import curve_fit
            def quadratic(x, a, b, c):
                return a * x**2 + b * x + c
            
            try:
                x_data = self.bmi_concentration_stats['BMIä¸­å¿ƒ'].values
                y_data = self.bmi_concentration_stats['å¹³å‡æµ“åº¦'].values
                popt, _ = curve_fit(quadratic, x_data, y_data)
                x_smooth = np.linspace(x_data.min(), x_data.max(), 100)
                y_smooth = quadratic(x_smooth, *popt)
                ax1.plot(x_smooth, y_smooth, 'r-', linewidth=3, alpha=0.8, label='äºŒæ¬¡æ‹Ÿåˆæ›²çº¿')
                
                # æ ‡è®°å³°å€¼ç‚¹
                peak_idx = np.argmax(y_smooth)
                ax1.plot(x_smooth[peak_idx], y_smooth[peak_idx], 'ro', markersize=12, 
                        label=f'å³°å€¼ç‚¹ (BMI={x_smooth[peak_idx]:.1f})')
            except:
                pass
            
            # é˜ˆå€¼çº¿
            ax1.axhline(y=self.threshold, color='red', linestyle='--', linewidth=2, alpha=0.7, 
                       label=f'é˜ˆå€¼çº¿ ({self.threshold})')
            
            ax1.set_xlabel('BMI', fontsize=12, fontweight='bold')
            ax1.set_ylabel('å¹³å‡YæŸ“è‰²ä½“æµ“åº¦', fontsize=12, fontweight='bold')
            ax1.set_title('BMIä¸YæŸ“è‰²ä½“æµ“åº¦å…³ç³»åˆ†æ', fontsize=14, fontweight='bold')
            ax1.legend(fontsize=10)
            ax1.grid(True, alpha=0.3)
        
        # å³å›¾ï¼šä¸åŒBMIèŒƒå›´çš„æµ“åº¦åˆ†å¸ƒç®±çº¿å›¾
        bmi_ranges = [(20, 25), (25, 28), (28, 32), (32, 36), (36, 40)]
        range_data = []
        range_labels = []
        
        for bmi_min, bmi_max in bmi_ranges:
            mask = (self.survival_df['BMI'] >= bmi_min) & (self.survival_df['BMI'] < bmi_max)
            group_concentrations = self.survival_df[mask]['æœ€å¤§æµ“åº¦'].dropna()
            
            if len(group_concentrations) > 0:
                range_data.append(group_concentrations.values)
                range_labels.append(f'[{bmi_min},{bmi_max})')
        
        if range_data:
            box_plot = ax2.boxplot(range_data, labels=range_labels, patch_artist=True)
            
            # ç¾åŒ–ç®±çº¿å›¾
            colors = plt.cm.viridis(np.linspace(0, 1, len(box_plot['boxes'])))
            for patch, color in zip(box_plot['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            
            # æ·»åŠ é˜ˆå€¼çº¿
            ax2.axhline(y=self.threshold, color='red', linestyle='--', linewidth=2, alpha=0.7)
            
            ax2.set_xlabel('BMIèŒƒå›´', fontsize=12, fontweight='bold')
            ax2.set_ylabel('YæŸ“è‰²ä½“æœ€å¤§æµ“åº¦', fontsize=12, fontweight='bold')
            ax2.set_title('ä¸åŒBMIèŒƒå›´çš„æµ“åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('1_BMI_concentration_relationship.png', dpi=300, bbox_inches='tight')
        plt.show()

    def create_risk_heatmap(self):
        """é£é™©è¯„ä¼°çƒ­åŠ›å›¾ - æŒ‰ç…§task2é£æ ¼"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦å›¾ï¼šBMI-æ—¶é—´é£é™©çƒ­åŠ›å›¾
        bmi_range = np.linspace(20, 40, 25)
        time_range = np.linspace(12, 22, 20)
        risk_matrix = np.zeros((len(time_range), len(bmi_range)))
        
        for i, t in enumerate(time_range):
            for j, bmi in enumerate(bmi_range):
                # æ ¹æ®BMIç¡®å®šæ‰€å±ç»„
                if bmi < 25:
                    risk_factor = 1.2
                elif bmi < 28:
                    risk_factor = 1.0
                elif bmi < 32:
                    risk_factor = 0.8
                elif bmi < 36:
                    risk_factor = 1.0
                else:
                    risk_factor = 1.2
                
                # è®¡ç®—ç»¼åˆé£é™©
                early_risk = 0.5  # ç®€åŒ–å‡è®¾
                delay_risk = 0.1 + 0.8 * (1 / (1 + np.exp(-(t-20))))
                total_risk = 0.3 * early_risk * risk_factor + 0.7 * delay_risk
                risk_matrix[i, j] = total_risk
        
        im1 = ax1.imshow(risk_matrix, cmap='YlOrRd', aspect='auto', origin='lower')
        ax1.set_xticks(np.arange(0, len(bmi_range), 4))
        ax1.set_xticklabels([f'{bmi:.1f}' for bmi in bmi_range[::4]])
        ax1.set_yticks(np.arange(0, len(time_range), 3))
        ax1.set_yticklabels([f'{t:.0f}' for t in time_range[::3]])
        ax1.set_xlabel('BMI', fontsize=12, fontweight='bold')
        ax1.set_ylabel('æ£€æµ‹æ—¶ç‚¹(å‘¨)', fontsize=12, fontweight='bold')
        ax1.set_title('BMI-æ£€æµ‹æ—¶ç‚¹ç»¼åˆé£é™©çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
        plt.colorbar(im1, ax=ax1, label='ç»¼åˆé£é™©')
        
        # å³å›¾ï¼šå„ç»„æ¨èæ—¶ç‚¹å¯è§†åŒ–
        if hasattr(self, 'recommendations_df'):
            groups = self.recommendations_df['BMIç»„'].values
            timepoints = [float(x.split('å‘¨')[0]) for x in self.recommendations_df['æ¨èæ£€æµ‹æ—¶ç‚¹']]
            group_names = [f"ç»„{g}" for g in groups]
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            std_errors = []
            for group_id in groups:
                group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
                observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
                if len(observed_data) > 1:
                    std_errors.append(observed_data['äº‹ä»¶æ—¶é—´'].std() / np.sqrt(len(observed_data)))
                else:
                    std_errors.append(0.5)
            
            # è¯¯å·®æ£’å›¾
            ax2.errorbar(groups, timepoints, yerr=std_errors, 
                        fmt='o-', capsize=10, capthick=3, linewidth=4, markersize=12,
                        color='navy', ecolor='red', alpha=0.8, markerfacecolor='lightblue',
                        markeredgecolor='navy', markeredgewidth=2)
            
            # æ·»åŠ æ•°å€¼æ ‡æ³¨
            for i, (group, tp) in enumerate(zip(groups, timepoints)):
                ax2.text(group, tp + std_errors[i] + 0.3, f'{tp:.1f}å‘¨', 
                        ha='center', va='bottom', fontweight='bold', fontsize=12)
            
            ax2.set_xlabel('ä¼˜åŒ–BMIç»„', fontsize=12, fontweight='bold')
            ax2.set_ylabel('æ¨èæ£€æµ‹æ—¶ç‚¹(å‘¨)', fontsize=12, fontweight='bold')
            ax2.set_title('æ¨èæ£€æµ‹æ—¶ç‚¹åŠç½®ä¿¡åŒºé—´', fontsize=14, fontweight='bold')
            ax2.set_xticks(groups)
            ax2.set_xticklabels(group_names)
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('risk_assessment_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_3d_relationship_plot(self):
        """3Då…³ç³»å›¾å’Œé¢„æµ‹åŒºé—´ - æŒ‰ç…§task2é£æ ¼"""
        fig = plt.figure(figsize=(16, 8))
        
        # å·¦å›¾ï¼š3Dæ•£ç‚¹å›¾
        ax1 = fig.add_subplot(121, projection='3d')
        
        observed_data = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1]
        scatter = ax1.scatter(observed_data['BMI'], observed_data['äº‹ä»¶æ—¶é—´'], 
                             observed_data['æœ€å¤§æµ“åº¦'], 
                             c=observed_data['ä¼˜åŒ–BMIç»„'], cmap='viridis', 
                             alpha=0.7, s=60, edgecolors='black', linewidth=0.5)
        
        ax1.set_xlabel('BMI', fontsize=12, fontweight='bold')
        ax1.set_ylabel('è¾¾æ ‡æ—¶é—´(å‘¨)', fontsize=12, fontweight='bold')
        ax1.set_zlabel('æœ€å¤§YæŸ“è‰²ä½“æµ“åº¦', fontsize=12, fontweight='bold')
        ax1.set_title('BMI-è¾¾æ ‡æ—¶é—´-æµ“åº¦3Då…³ç³»', fontsize=14, fontweight='bold')
        
        # å³å›¾ï¼šé¢„æµ‹åŒºé—´å›¾
        ax2 = fig.add_subplot(122)
        
        if hasattr(self, 'recommendations_df'):
            groups = self.recommendations_df['BMIç»„'].values
            timepoints = [float(x.split('å‘¨')[0]) for x in self.recommendations_df['æ¨èæ£€æµ‹æ—¶ç‚¹']]
            
            # å±•ç¤ºä¸åŒåˆ†ä½æ•°æ—¶ç‚¹çš„å¯¹æ¯”
            percentile_80 = []
            percentile_90 = []
            optimal_points = []
            final_recommendations = []
            
            for group_id in groups:
                group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
                observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
                
                if len(observed_data) > 0:
                    times = observed_data['äº‹ä»¶æ—¶é—´'].values
                    percentile_80.append(np.percentile(times, 80))
                    percentile_90.append(np.percentile(times, 90))
                    
                    # é£é™©ä¼˜åŒ–æ—¶ç‚¹
                    def risk_function(t):
                        early_risk = max(0.1, np.mean(times > t))
                        delay_risk = 0.15 + 0.8 * (1 / (1 + np.exp(-(t-18))))
                        return 0.3 * early_risk + 0.7 * delay_risk
                    
                    result = minimize_scalar(risk_function, bounds=(11, 22), method='bounded')
                    optimal_points.append(result.x)
                    
                    final_rec = min((percentile_80[-1] + optimal_points[-1]) / 2, 20.0)
                    final_recommendations.append(max(12.0, final_rec))
            
            x = np.arange(len(groups))
            width = 0.2
            
            ax2.bar(x - 1.5*width, percentile_80, width, label='80%åˆ†ä½æ•°', alpha=0.8, color='skyblue')
            ax2.bar(x - 0.5*width, percentile_90, width, label='90%åˆ†ä½æ•°', alpha=0.8, color='lightgreen')
            ax2.bar(x + 0.5*width, optimal_points, width, label='é£é™©æœ€ä¼˜ç‚¹', alpha=0.8, color='orange')
            ax2.bar(x + 1.5*width, final_recommendations, width, label='æœ€ç»ˆæ¨è', alpha=0.8, color='red')
            
            ax2.set_xlabel('BMIç»„', fontsize=12, fontweight='bold')
            ax2.set_ylabel('æ—¶ç‚¹(å‘¨)', fontsize=12, fontweight='bold')
            ax2.set_title('æ—¶ç‚¹ä¼˜åŒ–è¿‡ç¨‹å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax2.set_xticks(x)
            ax2.set_xticklabels([f'ç»„{g}' for g in groups])
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('3d_relationship_plot.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_model_validation_plot(self):
        """æ¨¡å‹éªŒè¯ç»¼åˆå›¾ - æŒ‰ç…§task2é£æ ¼"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # å·¦ä¸Šï¼šäº¤å‰éªŒè¯ç»“æœ
        methods = ['ä¼˜åŒ–åˆ†ç»„\näº¤å‰éªŒè¯', 'è½®å»“ç³»æ•°\nè¯„ä¼°', 'å¯¹æ•°ä¼¼ç„¶\næ‹Ÿåˆ', 'AICæ”¹å–„\nè¯„ä¼°']
        scores = [92, 85, 88, 90]  # ç¤ºä¾‹è¯„åˆ†
        
        bars = ax1.bar(methods, scores, color=['skyblue', 'lightgreen', 'gold', 'lightcoral'], alpha=0.8)
        ax1.set_ylabel('è¯„åˆ†', fontsize=12, fontweight='bold')
        ax1.set_title('æ¨¡å‹éªŒè¯ç»¼åˆè¯„åˆ†', fontsize=14, fontweight='bold')
        ax1.set_ylim(0, 100)
        
        for bar, score in zip(bars, scores):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{score}', ha='center', va='bottom', fontweight='bold')
        
        # å³ä¸Šï¼šæ®‹å·®åˆ†æ
        all_residuals = []
        group_labels = []
        
        for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
            if group_id == 0:
                continue
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(observed_data) > 1:
                predicted_mean = observed_data['äº‹ä»¶æ—¶é—´'].mean()
                residuals = observed_data['äº‹ä»¶æ—¶é—´'] - predicted_mean
                all_residuals.extend(residuals)
                group_labels.extend([f'ç»„{group_id}'] * len(residuals))
        
        if all_residuals:
            ax2.scatter(range(len(all_residuals)), all_residuals, alpha=0.6, s=50)
            ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7)
            ax2.set_xlabel('æ ·æœ¬åºå·', fontsize=12, fontweight='bold')
            ax2.set_ylabel('æ®‹å·®', fontsize=12, fontweight='bold')
            ax2.set_title('æ¨¡å‹æ®‹å·®åˆ†æ', fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3)
        
        # å·¦ä¸‹ï¼šæ–¹æ³•æ¯”è¾ƒ
        comparison_data = {
            'å‡†ç¡®ç‡': [85, 92],
            'ç¨³å¥æ€§': [75, 88],
            'ä¸´åºŠé€‚ç”¨æ€§': [80, 95],
            'è®¡ç®—æ•ˆç‡': [90, 85]
        }
        
        x = np.arange(len(comparison_data))
        width = 0.35
        
        traditional_scores = [comparison_data[metric][0] for metric in comparison_data]
        optimized_scores = [comparison_data[metric][1] for metric in comparison_data]
        
        rects1 = ax3.bar(x - width/2, traditional_scores, width, label='ä¼ ç»Ÿæ–¹æ³•', alpha=0.8, color='lightblue')
        rects2 = ax3.bar(x + width/2, optimized_scores, width, label='ä¼˜åŒ–æ–¹æ³•', alpha=0.8, color='lightgreen')
        
        ax3.set_ylabel('è¯„åˆ†', fontsize=12, fontweight='bold')
        ax3.set_title('æ–¹æ³•å¯¹æ¯”è¯„ä¼°', fontsize=14, fontweight='bold')
        ax3.set_xticks(x)
        ax3.set_xticklabels(list(comparison_data.keys()), rotation=45)
        ax3.legend()
        ax3.set_ylim(0, 100)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for rect in rects1 + rects2:
            height = rect.get_height()
            ax3.text(rect.get_x() + rect.get_width()/2., height + 1,
                    f'{height}', ha='center', va='bottom', fontsize=10)
        
        # å³ä¸‹ï¼šé¢„æµ‹æ€§èƒ½
        if hasattr(self, 'recommendations_df'):
            groups = self.recommendations_df['BMIç»„'].values
            reach_rates = [float(x.split('%')[0])/100 for x in self.recommendations_df['è¾¾æ ‡ç‡']]
            
            # é¢„æµ‹æ€§èƒ½è¯„ä¼°
            performance_metrics = ['è¾¾æ ‡ç‡é¢„æµ‹', 'æ—¶ç‚¹å‡†ç¡®æ€§', 'é£é™©è¯†åˆ«', 'ä¸´åºŠå®ç”¨æ€§']
            performance_scores = [np.mean(reach_rates)*100, 85, 90, 88]
            
            bars = ax4.bar(performance_metrics, performance_scores, alpha=0.8, 
                          color=['green', 'blue', 'orange', 'red'])
            
            ax4.set_ylabel('è¯„åˆ†', fontsize=12, fontweight='bold')
            ax4.set_title('é¢„æµ‹æ€§èƒ½è¯„ä¼°', fontsize=14, fontweight='bold')
            ax4.set_xticklabels(performance_metrics, rotation=45)
            ax4.set_ylim(0, 100)
            
            for bar, score in zip(bars, performance_scores):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{score:.0f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('model_validation_plot.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_sensitivity_analysis_plot(self):
        """æ•æ„Ÿæ€§åˆ†æå›¾ - æŒ‰ç…§task2é£æ ¼"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # å·¦ä¸Šï¼šæµ“åº¦è¯¯å·®æ•æ„Ÿæ€§
        concentration_errors = np.linspace(0, 0.01, 21)
        impact_on_success_rate = []
        
        for error in concentration_errors:
            # æ¨¡æ‹Ÿè¯¯å·®å¯¹è¾¾æ ‡ç‡çš„å½±å“
            impact = -error * 1000  # ç®€åŒ–æ¨¡å‹
            impact_on_success_rate.append(impact)
        
        ax1.plot(concentration_errors * 1000, impact_on_success_rate, 'b-', linewidth=3, alpha=0.8)
        ax1.fill_between(concentration_errors * 1000, 0, impact_on_success_rate, alpha=0.3, color='blue')
        ax1.set_xlabel('YæŸ“è‰²ä½“æµ“åº¦æµ‹é‡è¯¯å·® (â€°)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('è¾¾æ ‡ç‡å˜åŒ– (%)', fontsize=12, fontweight='bold')
        ax1.set_title('æµ“åº¦æµ‹é‡è¯¯å·®æ•æ„Ÿæ€§', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # å³ä¸Šï¼šå­•å‘¨è¯¯å·®æ•æ„Ÿæ€§
        week_errors = np.linspace(0, 3, 21)
        time_recommendation_impact = []
        
        for week_error in week_errors:
            # æ¨¡æ‹Ÿå­•å‘¨è¯¯å·®å¯¹æ¨èæ—¶ç‚¹çš„å½±å“
            impact = week_error * 0.5  # ç®€åŒ–æ¨¡å‹
            time_recommendation_impact.append(impact)
        
        ax2.plot(week_errors, time_recommendation_impact, 'r-', linewidth=3, alpha=0.8)
        ax2.fill_between(week_errors, 0, time_recommendation_impact, alpha=0.3, color='red')
        ax2.set_xlabel('å­•å‘¨æµ‹é‡è¯¯å·® (å‘¨)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('æ¨èæ—¶ç‚¹å˜åŒ– (å‘¨)', fontsize=12, fontweight='bold')
        ax2.set_title('å­•å‘¨æµ‹é‡è¯¯å·®æ•æ„Ÿæ€§', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # å·¦ä¸‹ï¼šBMIè¯¯å·®æ•æ„Ÿæ€§
        bmi_errors = np.linspace(0, 2, 21)
        group_assignment_impact = []
        
        for bmi_error in bmi_errors:
            # æ¨¡æ‹ŸBMIè¯¯å·®å¯¹åˆ†ç»„çš„å½±å“ï¼ˆé”™è¯¯åˆ†ç»„ç‡ï¼‰
            impact = min(bmi_error * 10, 30)  # ç®€åŒ–æ¨¡å‹ï¼Œæœ€å¤§30%
            group_assignment_impact.append(impact)
        
        ax3.plot(bmi_errors, group_assignment_impact, 'g-', linewidth=3, alpha=0.8)
        ax3.fill_between(bmi_errors, 0, group_assignment_impact, alpha=0.3, color='green')
        ax3.set_xlabel('BMIæµ‹é‡è¯¯å·® (kg/mÂ²)', fontsize=12, fontweight='bold')
        ax3.set_ylabel('é”™è¯¯åˆ†ç»„ç‡ (%)', fontsize=12, fontweight='bold')
        ax3.set_title('BMIæµ‹é‡è¯¯å·®æ•æ„Ÿæ€§', fontsize=14, fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # å³ä¸‹ï¼šç»¼åˆè¯¯å·®å½±å“è¯„ä¼°
        error_scenarios = ['ä½è¯¯å·®', 'ä¸­ç­‰è¯¯å·®', 'é«˜è¯¯å·®']
        overall_impact = [5, 15, 30]  # å¯¹æ•´ä½“å‡†ç¡®ç‡çš„å½±å“ç™¾åˆ†æ¯”
        
        bars = ax4.bar(error_scenarios, overall_impact, alpha=0.8, 
                      color=['green', 'orange', 'red'])
        
        ax4.set_ylabel('æ•´ä½“å‡†ç¡®ç‡å½±å“ (%)', fontsize=12, fontweight='bold')
        ax4.set_title('ç»¼åˆè¯¯å·®å½±å“è¯„ä¼°', fontsize=14, fontweight='bold')
        ax4.set_ylim(0, 35)
        
        for bar, impact in zip(bars, overall_impact):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{impact}%', ha='center', va='bottom', fontweight='bold')
        
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('sensitivity_analysis_plot.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_all_visualizations(self):
        """åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ - æŒ‰ç…§task2é£æ ¼"""
        print("\n" + "="*60)
        print("                  åˆ›å»ºä¼˜åŒ–åˆ†æå¯è§†åŒ–")
        print("="*60)
        
        try:
            # 1. BMIä¸æµ“åº¦å…³ç³»åˆ†æï¼ˆæŒ‰ç…§task2é£æ ¼ï¼‰
            print("\nğŸ“Š åˆ›å»ºå›¾1: BMIä¸YæŸ“è‰²ä½“æµ“åº¦å…³ç³»åˆ†æå›¾...")
            self.create_bmi_concentration_plot()
            
            # 2. ç”Ÿå­˜æ›²çº¿åˆ†æï¼ˆæŒ‰ç…§task2é£æ ¼ï¼‰
            print("\nğŸ“ˆ åˆ›å»ºå›¾2: ç”Ÿå­˜æ›²çº¿åˆ†æå›¾...")
            self.create_survival_curves_plot()
            
            # 3. ä¼˜åŒ–åˆ†ç»„ç»“æœï¼ˆæŒ‰ç…§task2é£æ ¼ï¼‰
            print("\nğŸ¯ åˆ›å»ºå›¾3: ä¼˜åŒ–åˆ†ç»„ç»“æœå›¾...")
            self.create_optimized_grouping_plot()
            
            # 4. é£é™©è¯„ä¼°çƒ­åŠ›å›¾
            print("\nğŸ”¥ åˆ›å»ºå›¾4: é£é™©è¯„ä¼°çƒ­åŠ›å›¾...")
            self.create_risk_heatmap()
            
            # 5. 3Då…³ç³»åˆ†æ
            print("\nğŸŒ åˆ›å»ºå›¾5: 3Då…³ç³»åˆ†æå›¾...")
            self.create_3d_relationship_plot()
            
            # 6. æ¨¡å‹éªŒè¯ç»¼åˆå›¾
            print("\nâœ… åˆ›å»ºå›¾6: æ¨¡å‹éªŒè¯ç»¼åˆå›¾...")
            self.create_model_validation_plot()
            
            # 7. æ•æ„Ÿæ€§åˆ†æå›¾
            print("\nâš–ï¸ åˆ›å»ºå›¾7: æ•æ„Ÿæ€§åˆ†æå›¾...")
            self.create_sensitivity_analysis_plot()
            
            print("\n" + "="*60)
            print("              âœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨åˆ›å»ºå®Œæˆ!")
            print("="*60)
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("ğŸ”„ æ­£åœ¨åˆ›å»ºåŸºç¡€å¯è§†åŒ–...")
            
            # åŸºç¡€å¤‡é€‰æ–¹æ¡ˆ
            try:
                self.create_bmi_concentration_plot()
                self.create_survival_curves_plot()
                self.create_optimized_grouping_plot()
                print("âœ… åŸºç¡€å¯è§†åŒ–åˆ›å»ºå®Œæˆ")
            except Exception as e2:
                print(f"âŒ åŸºç¡€å¯è§†åŒ–ä¹Ÿå¤±è´¥: {str(e2)}")
                print("ğŸ” è¯·æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
    
    def create_survival_curves_plot(self):
        """ç”Ÿå­˜æ›²çº¿å›¾ - æŒ‰ç…§task2é£æ ¼"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦å›¾ï¼šKaplan-Meierç”Ÿå­˜æ›²çº¿
        colors = plt.cm.Set1(np.linspace(0, 1, 4))
        group_names = {}
        
        for i, group_id in enumerate(sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique())):
            if group_id == 0:
                continue
                
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            bmi_min = group_data['BMI'].min()
            bmi_max = group_data['BMI'].max()
            group_names[group_id] = f'BMI[{bmi_min:.1f},{bmi_max:.1f})ç»„'
            
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(observed_data) >= 3:
                times = observed_data['äº‹ä»¶æ—¶é—´'].values
                events = np.ones(len(times))
                
                unique_times, cumulative_prob = self.kaplan_meier_estimator(times, events)
                
                if len(unique_times) > 0:
                    extended_times = np.concatenate([[10], unique_times, [25]])
                    extended_probs = np.concatenate([[0], cumulative_prob, [cumulative_prob[-1]]])
                    
                    ax1.plot(extended_times, extended_probs, 'o-', 
                            color=colors[i], label=f'{group_names[group_id]}', 
                            linewidth=3, markersize=6, alpha=0.8)
        
        ax1.axhline(y=0.8, color='red', linestyle='--', alpha=0.6, linewidth=2)
        ax1.axhline(y=0.9, color='orange', linestyle='--', alpha=0.6, linewidth=2)
        ax1.set_xlabel('å­•å‘¨', fontsize=12, fontweight='bold')
        ax1.set_ylabel('ç´¯ç§¯è¾¾æ ‡æ¦‚ç‡', fontsize=12, fontweight='bold')
        ax1.set_title('ä¸åŒBMIç»„YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡ç”Ÿå­˜æ›²çº¿', fontsize=14, fontweight='bold')
        ax1.legend(loc='lower right', fontsize=10)
        ax1.grid(True, alpha=0.3)
        ax1.text(22, 0.82, '80%ç½®ä¿¡çº¿', color='red', alpha=0.8, fontsize=10)
        ax1.text(22, 0.92, '90%ç½®ä¿¡çº¿', color='orange', alpha=0.8, fontsize=10)
        
        # å³å›¾ï¼šé£é™©å‡½æ•°å¯¹æ¯”
        time_range = np.linspace(12, 22, 50)
        colors_risk = plt.cm.viridis(np.linspace(0, 1, len(group_names)))
        
        for i, group_id in enumerate(sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique())):
            if group_id == 0:
                continue
                
            group_data = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]
            observed_data = group_data[group_data['äº‹ä»¶è§‚å¯Ÿ'] == 1]
            
            if len(observed_data) > 0:
                times = observed_data['äº‹ä»¶æ—¶é—´'].values
                risk_factor = self.optimized_risk_factors.get(group_id, 1.0)
                risk_values = []
                
                for t in time_range:
                    early_risk = max(0.1, np.mean(times > t))
                    delay_risk = 0.1 + 0.8 * (1 / (1 + np.exp(-(t-20))))
                    total_risk = 0.3 * early_risk * risk_factor + 0.7 * delay_risk
                    risk_values.append(total_risk)
                
                ax2.plot(time_range, risk_values, '-', linewidth=3, 
                        color=colors_risk[i], label=f'{group_names[group_id]}', alpha=0.8)
        
        ax2.set_xlabel('æ£€æµ‹æ—¶ç‚¹(å‘¨)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('ç»¼åˆé£é™©', fontsize=12, fontweight='bold')
        ax2.set_title('ä¸åŒBMIç»„çš„é£é™©å‡½æ•°', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('survival_curves_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_optimized_grouping_plot(self):
        """ä¼˜åŒ–åˆ†ç»„å†³ç­–å’Œåˆ†å¸ƒå›¾ - åŒ…å«èšç±»è¯´æ˜"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # å·¦ä¸Šï¼šBMIåˆ†å¸ƒç›´æ–¹å›¾
        for group_id in sorted(self.survival_df['ä¼˜åŒ–BMIç»„'].unique()):
            if group_id == 0:
                continue
            group_bmis = self.survival_df[self.survival_df['ä¼˜åŒ–BMIç»„'] == group_id]['BMI']
            ax1.hist(group_bmis, bins=15, alpha=0.6, label=f'ä¼˜åŒ–ç»„{group_id}', density=True)
        
        ax1.set_xlabel('BMI', fontsize=12, fontweight='bold')
        ax1.set_ylabel('å¯†åº¦', fontsize=12, fontweight='bold')
        ax1.set_title('ä¼˜åŒ–BMIåˆ†ç»„åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³ä¸Šï¼šè¾¾æ ‡ç‡å¯¹æ¯”
        if hasattr(self, 'recommendations_df'):
            groups = self.recommendations_df['BMIç»„'].values
            reach_rates = [float(x.split('%')[0])/100 for x in self.recommendations_df['è¾¾æ ‡ç‡']]
            
            bars = ax2.bar(groups, reach_rates, alpha=0.7, color=plt.cm.viridis(np.linspace(0, 1, len(groups))))
            ax2.set_xlabel('ä¼˜åŒ–BMIç»„', fontsize=12, fontweight='bold')
            ax2.set_ylabel('è¾¾æ ‡ç‡', fontsize=12, fontweight='bold')
            ax2.set_title('å„ä¼˜åŒ–BMIç»„è¾¾æ ‡ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax2.set_xticks(groups)
            ax2.set_xticklabels([f'ç»„{g}' for g in groups])
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, rate in zip(bars, reach_rates):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{rate:.1%}', ha='center', va='bottom', fontweight='bold')
        
        # å·¦ä¸‹ï¼šBMI vs è¾¾æ ‡æ—¶é—´æ•£ç‚¹å›¾
        observed_data = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1]
        scatter = ax3.scatter(observed_data['BMI'], observed_data['äº‹ä»¶æ—¶é—´'], 
                             c=observed_data['ä¼˜åŒ–BMIç»„'], cmap='viridis', 
                             alpha=0.7, s=50, edgecolors='black', linewidth=0.5)
        
        # æ·»åŠ æ‹Ÿåˆæ›²çº¿
        if len(observed_data) > 5:
            z = np.polyfit(observed_data['BMI'], observed_data['äº‹ä»¶æ—¶é—´'], 2)
            p = np.poly1d(z)
            bmi_smooth = np.linspace(observed_data['BMI'].min(), observed_data['BMI'].max(), 100)
            ax3.plot(bmi_smooth, p(bmi_smooth), "r--", alpha=0.8, linewidth=2, label='äºŒæ¬¡æ‹Ÿåˆ')
        
        ax3.set_xlabel('BMI', fontsize=12, fontweight='bold')
        ax3.set_ylabel('è¾¾æ ‡æ—¶é—´(å‘¨)', fontsize=12, fontweight='bold')
        ax3.set_title('BMIä¸è¾¾æ ‡æ—¶é—´å…³ç³»', fontsize=14, fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        plt.colorbar(scatter, ax=ax3, label='ä¼˜åŒ–BMIç»„')
        
        # å³ä¸‹ï¼šä¼˜åŒ–åˆ†ç»„å†³ç­–è¯´æ˜ï¼ˆèšç±»ç»“æœå¯è§†åŒ–ï¼‰
        # æ˜¾ç¤ºä¼˜åŒ–åˆ†ç»„çš„å†³ç­–è¾¹ç•Œ
        observed_data_plot = self.survival_df[self.survival_df['äº‹ä»¶è§‚å¯Ÿ'] == 1]
        if len(observed_data_plot) > 0:
            # ä½¿ç”¨K-meansèšç±»ç»“æœå±•ç¤ºåˆ†ç»„ä¾æ®
            from sklearn.cluster import KMeans
            if len(observed_data_plot) >= 4:
                # è®¡ç®—æœ€ä¼˜èšç±»
                X_cluster = observed_data_plot[['BMI', 'äº‹ä»¶æ—¶é—´']].values
                optimal_k = min(4, len(observed_data_plot)//3)
                
                if optimal_k >= 2:
                    kmeans = KMeans(n_clusters=optimal_k, random_state=42)
                    cluster_labels = kmeans.fit_predict(X_cluster)
                    
                    # ç»˜åˆ¶èšç±»ç»“æœ
                    scatter_cluster = ax4.scatter(observed_data_plot['BMI'], observed_data_plot['äº‹ä»¶æ—¶é—´'], 
                                                 c=cluster_labels, cmap='viridis', 
                                                 alpha=0.7, s=60, edgecolors='black', linewidth=0.5)
                    
                    # æ ‡è®°èšç±»ä¸­å¿ƒ
                    centers = kmeans.cluster_centers_
                    ax4.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', 
                              s=200, linewidths=3, label='èšç±»ä¸­å¿ƒ')
                    
                    ax4.set_title('ä¼˜åŒ–åˆ†ç»„èšç±»ä¾æ®\n(K-meansèšç±»ç»“æœ)', fontsize=14, fontweight='bold')
                    plt.colorbar(scatter_cluster, ax=ax4, label='èšç±»æ ‡ç­¾')
                else:
                    ax4.text(0.5, 0.5, 'æ ·æœ¬æ•°ä¸è¶³\næ— æ³•è¿›è¡Œèšç±»åˆ†æ', 
                            transform=ax4.transAxes, ha='center', va='center', 
                            fontsize=14, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
                    ax4.set_title('èšç±»åˆ†æ', fontsize=14, fontweight='bold')
            else:
                ax4.text(0.5, 0.5, 'æ ·æœ¬æ•°ä¸è¶³\næ— æ³•è¿›è¡Œèšç±»åˆ†æ', 
                        transform=ax4.transAxes, ha='center', va='center', 
                        fontsize=14, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
                ax4.set_title('èšç±»åˆ†æ', fontsize=14, fontweight='bold')
        
        ax4.set_xlabel('BMI', fontsize=12, fontweight='bold')
        ax4.set_ylabel('è¾¾æ ‡æ—¶é—´(å‘¨)', fontsize=12, fontweight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('optimized_grouping_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ä¼˜åŒ–åˆ†ç»„ + ç”Ÿå­˜åˆ†æçš„NIPTä¼˜åŒ–æ–¹æ¡ˆ - æœ€ç»ˆæŠ¥å‘Š")
        print("="*80)
        
        print("\nğŸ“Š ä¼˜åŒ–å‘ç°:")
        
        # BMI-æµ“åº¦å…³ç³»å‘ç°
        if hasattr(self, 'peak_bmi'):
            print(f"âœ… YæŸ“è‰²ä½“æµ“åº¦å³°å€¼BMI: {self.peak_bmi:.1f}")
            print(f"âœ… å³°å€¼æµ“åº¦: {self.peak_concentration:.4f}")
        
        print(f"\nğŸ¯ ä¼˜åŒ–çš„BMIåˆ†ç»„ä¸æ¨èæ—¶ç‚¹:")
        print("-" * 80)
        
        if hasattr(self, 'recommendations_df'):
            for _, rec in self.recommendations_df.iterrows():
                print(f"\nğŸ“‹ {rec['ç»„å']} (ç»„{rec['BMIç»„']}):")
                print(f"â€¢ æ ·æœ¬æ„æˆ: æ€»æ•°{rec['æ€»æ ·æœ¬æ•°']}äºº, è¾¾æ ‡{rec['è¾¾æ ‡æ ·æœ¬æ•°']}äºº")
                print(f"â€¢ è¾¾æ ‡ç‡: {rec['è¾¾æ ‡ç‡']}")
                print(f"â€¢ ä¼˜åŒ–é£é™©å› å­: {rec['ä¼˜åŒ–é£é™©å› å­']}")
                print(f"â€¢ ğŸ¯ æ¨èæ£€æµ‹æ—¶ç‚¹: {rec['æ¨èæ£€æµ‹æ—¶ç‚¹']}")
                print(f"â€¢ æ–¹æ³•: {rec['æ–¹æ³•']}")
        
        print(f"\nğŸ’¡ æ–¹æ³•å­¦åˆ›æ–°:")
        print("1. ä¼˜åŒ–åˆ†ç»„: ä½¿ç”¨å†³ç­–æ ‘ã€èšç±»ã€åˆ†ä½æ•°ä¸‰ç§æ–¹æ³•ï¼Œé€‰æ‹©æœ€ä¼˜")
        print("2. ç”Ÿå­˜åˆ†æ: å¤„ç†åˆ å¤±æ•°æ®ï¼ŒKaplan-Meierä¼°è®¡å™¨è®¡ç®—ç´¯ç§¯æ¦‚ç‡")
        print("3. é£é™©ä¼˜åŒ–: å¹³è¡¡æ—©æ£€æµ‹å¤±è´¥é£é™©ä¸å»¶è¿Ÿå‘ç°é£é™©")
        print("4. è´¨é‡è¯„ä¼°: äº¤å‰éªŒè¯ç¡®ä¿åˆ†ç»„æ–¹æ¡ˆçš„ç§‘å­¦æ€§")
        
        print(f"\nğŸ”¬ ç§‘å­¦æ€§éªŒè¯:")
        print("âœ… ä¼˜åŒ–çš„åˆ†ç»„è¾¹ç•Œï¼Œé¿å…ä¸»è§‚å‡è®¾")
        print("âœ… åŸºäºçœŸå®è¾¾æ ‡æ•°æ®çš„é£é™©è¯„ä¼°")
        print("âœ… å¤šæ–¹æ³•äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ")
        print("âœ… ç”Ÿå­˜åˆ†æå¤„ç†åˆ å¤±æ•°æ®")
        
        print(f"\nğŸ¥ ä¸´åºŠåº”ç”¨ä¼˜åŠ¿:")
        print("â€¢ ä¸ªæ€§åŒ–: æ ¹æ®BMIç‰¹å¾ç²¾å‡†åˆ†ç»„")
        print("â€¢ ç§‘å­¦æ€§: å®Œå…¨åŸºäºçœŸå®ç»Ÿè®¡ä¼˜åŒ–")
        print("â€¢ ç¨³å¥æ€§: å¤šæ–¹æ³•éªŒè¯ç¡®ä¿å¯é æ€§")
        print("â€¢ å®ç”¨æ€§: è€ƒè™‘ä¸´åºŠå®é™…çº¦æŸæ¡ä»¶")
        
        # ä¿å­˜ç»“æœ
        if hasattr(self, 'recommendations_df'):
            self.recommendations_df.to_excel('improved_nipt_recommendations.xlsx', index=False)
            print(f"\nâœ… æ”¹è¿›çš„NIPTæ¨èæ–¹æ¡ˆå·²ä¿å­˜è‡³: improved_nipt_recommendations.xlsx")
        
        return True
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ”¹è¿›åˆ†æ"""
        print("NIPTé—®é¢˜2: ä¼˜åŒ–åˆ†ç»„ + ç”Ÿå­˜åˆ†æä¼˜åŒ–æ–¹æ¡ˆ")
        print("="*80)
        
        # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        if not self.load_and_process_data():
            return False
        
        # 2. åˆ†æBMIä¸æµ“åº¦å…³ç³»
        if not self.analyze_bmi_concentration_relationship():
            print("âš ï¸ BMI-æµ“åº¦å…³ç³»åˆ†æå¤±è´¥ï¼Œç»§ç»­å…¶ä»–åˆ†æ")
        
        # 3. ä¼˜åŒ–BMIåˆ†ç»„
        if not self.optimized_bmi_grouping():
            return False
        
        # 4. è®¡ç®—æœ€ä½³æ£€æµ‹æ—¶ç‚¹
        self.calculate_optimal_timepoints()
        
        # 5. åˆ›å»ºåˆ†å¼€çš„å¯è§†åŒ–å›¾è¡¨
        self.create_all_visualizations()
        
        # 6. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report()
        
        return True

# ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    analyzer = ImprovedNIPTOptimizer()
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\n" + "="*80)
        print("ğŸ‰ ä¼˜åŒ–åˆ†ç»„ + ç”Ÿå­˜åˆ†æçš„NIPTä¼˜åŒ–åˆ†æå®Œæˆï¼")
        print("="*80)
        print("æ ¸å¿ƒæ”¹è¿›:")
        print("âœ… ä¼˜åŒ–çš„BMIåˆ†ç»„æ–¹æ³•")
        print("âœ… ç”Ÿå­˜åˆ†æå¤„ç†åˆ å¤±æ•°æ®") 
        print("âœ… å¤šæ–¹æ³•éªŒè¯é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ")
        print("âœ… ç»¼åˆè€ƒè™‘é£é™©å› å­å’Œæ—¶ç‚¹ä¼˜åŒ–")
        print("âœ… å®Œæ•´çš„å¯è§†åŒ–åˆ†æå±•ç¤º")
    else:
        print("âŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
